[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Marek Prokop",
    "section": "",
    "text": "Jmenuju se Marek Prokop a pracuju jako konzultant a analytik digitálního marketingu ve své firmě PROKOP software s.r.o. Kdysi dávno jsem programoval, a přestože mě už přes 20 let živí něco jiného, slovo software jsem v názvu firmy nechal. I v marketingu totiž rád vymýšlím řešení, která jdou „naprogramovat“ – ať už se jedná o různé analýzy, marketingovou automatizaci, nebo tvorbu webů.\nA právě taková řešení si zapisuju do tohoto blogu. Zapisuju si je sem především pro sebe, abych je tu snadno našel, když je zase zapomenu, ale budu rád, když přinesou nějaký užitek i vám.\nVětšina řešení je v jazyce R, a jestli je chcete využít pro sebe, měli byste R alespoň malinko umět. Když neumíte, projděte si mou příručku Od Excelu k R a budete připravení. Kromě R občas použiju i Google Sheets, Google Apps Script, Python apod. Pokusím se zápisky štítkovat, aby bylo jasné, o čem konkrétně jsou."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Nejnovější zápisky",
    "section": "",
    "text": "Analýza struktury většího počtu URL\n\n\n\n\n\nKdyž dostanu do ruky nový web, zajímá mě z jakých URL se skládá. Postupy popsané v tomhle zápisku mi to pomáhají zjistit a přehledně zobrazit.\n\n\n\n\n\n\nFeb 23, 2023\n\n\nMarek Prokop\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDěláte digitální marketing a chcete začít používat R? Tohle je váš studijní plán\n\n\n\n\n\nNaučit se používat R v digitálním marketingu je docela lehké, ale těžké je vybrat, co se vlastně máte učit. Bez dobrého plánu vás čeká hodně slepých uliček. Tohle je ten dobrý plán :-)\n\n\n\n\n\n\nFeb 17, 2023\n\n\nMarek Prokop\n\n\n\n\n\n\n  \n\n\n\n\nDetekce změny průměru v časové řadě\n\n\n\n\n\nUkážu, jak jde v R detekovat a hezky vykreslit významné změny průměru v časové řadě. Za příklad mi poslouží imprese ze Search Console.\n\n\n\n\n\n\nMay 16, 2022\n\n\nMarek Prokop\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJak se zbavit azbuky, čínštiny apod.\n\n\n\n\n\nDo dat se občas dostane nechtěná ruština, čínština, řečtina, nebo jiný jazyk psaný nelatinkovým písmem. Tohle je rychlý návod, jak se takových textů pomocí regulárních výrazů zbavit, nebo je naopak najít.\n\n\n\n\n\n\nMay 1, 2022\n\n\nMarek Prokop\n\n\n\n\n\n\n  \n\n\n\n\nČasové řady z dat Search Console\n\n\n\n\n\nJako první na vás v Search Consoli vyskočí graf výkonu webu v čase. Časové řady ze Search Console jdou ale vizualizovat o dost lépe a jdou z nich pak vypozorovat zajímavé věci.\n\n\n\n\n\n\nApr 18, 2022\n\n\nMarek Prokop\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nČístím Search Consoli od zbytečných webů\n\n\n\n\n\nJako každý SEO konzultantant mám v Google Search Consoli plno webů, ke kterým už nemám přístup, nebo které už dlouho nefungují. Napsal jsem si skript v R, který je odstraní.\n\n\n\n\n\n\nApr 10, 2022\n\n\nMarek Prokop\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKontrola stavových kódů většího počtu URL v R\n\n\n\n\n\nPotřeboval jsem rychle zkontrolovat stavové kódy HTTP dlouhého seznamu URL. Přitom jsem si vyzkoušel balíček furrr na paralelní zpracování a moc se mi zalíbil.\n\n\n\n\n\n\nApr 9, 2022\n\n\nMarek Prokop\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2022-04-09-cistim-search-consoli/index.html",
    "href": "posts/2022-04-09-cistim-search-consoli/index.html",
    "title": "Čístím Search Consoli od zbytečných webů",
    "section": "",
    "text": "Skriptu stačí tyhle dva balíčky.\n\nlibrary(tidyverse)\nlibrary(searchConsoleR)\n\nA klasická autorizace do Search Console. Kdybych neměl svůj Google e-mail uložený v systémové proměnné, musel bych ho napsat do parametru email přímo.\n\nscr_auth(email = Sys.getenv(\"MY_GOOGLE_ACCOUNT\"))\n\n\n\n\nFunkcí list_websites načtu všechny weby, které v Search Consoli mám, a uložím si je do objektu websites.\n\nwebsites <- list_websites()\n\nSeznam vypadá nějak takhle, jen jsem konkrétní weby ze své Search Console anonymizoval, abych mohl výstup publikovat. Když vynechám poslední řádek, budou weby normálně vidět.\n\nwebsites |> \n  slice_sample(n = 10) |> \n  mutate(siteUrl = anonymize_site(siteUrl))\n\n                    siteUrl    permissionLevel\n1    http://anonymized.com/       siteFullUser\n2    https://anonymized.cz/       siteFullUser\n3   sc-domain:anonymized.cz       siteFullUser\n4   sc-domain:anonymized.cz siteRestrictedUser\n5   https://anonymized.cz/d       siteFullUser\n6    http://anonymized.cz/p       siteFullUser\n7   https://anonymized.cz/e       siteFullUser\n8   https://anonymized.cz/c       siteFullUser\n9   sc-domain:anonymized.sk       siteFullUser\n10 sc-domain:anonymized.com          siteOwner"
  },
  {
    "objectID": "posts/2022-04-09-cistim-search-consoli/index.html#neautorizované-weby",
    "href": "posts/2022-04-09-cistim-search-consoli/index.html#neautorizované-weby",
    "title": "Čístím Search Consoli od zbytečných webů",
    "section": "Neautorizované weby",
    "text": "Neautorizované weby\nDůležitý je sloupec permissionLevel. Mám v něm tyhle hodnoty:\n\nwebsites |> \n  count(permissionLevel)\n\n     permissionLevel  n\n1       siteFullUser 67\n2          siteOwner 32\n3 siteRestrictedUser 17\n4 siteUnverifiedUser  3\n\n\nZbavit se chci webů, ke kterým nemám povolený přístup. Vypíšu si je a raději je pečlivě zkontroluju (samozřejmě až odstraním anonymizaci na posledním řádku).\n\nunverified_websites <- websites |> \n  filter(permissionLevel == \"siteUnverifiedUser\")\n\nunverified_websites |> \n  mutate(siteUrl = anonymize_site(siteUrl))\n\n                  siteUrl    permissionLevel\n1 sc-domain:anonymized.cz siteUnverifiedUser\n2   http://anonymized.sk/ siteUnverifiedUser\n3   http://anonymized.cz/ siteUnverifiedUser\n\n\nZe Search Console si je pak odstraním takto. Jen před tím přepíšu na prvním řádku FALSE na TRUE. Dal jsem to tam proto, abych si nechtěně nesmazal weby před tím, než si je zkontroluju.\nif (FALSE) {\n  unverified_websites |> \n    pull(siteUrl) |> \n    walk(delete_website)\n}\nKdyž to pustím, křičí to na mě nějaké warningy ohledně JSON. Asi je někde nějaká chybka, ale podle všeho ničemu nevadí a skript dělá to, co má. Každopádně si pak můžu ověřit, jestli už jsou všechny neverifikované weby pryč:\nlist_websites() |> \n  filter(permissionLevel == \"siteUnverifiedUser\")"
  },
  {
    "objectID": "posts/2022-04-09-cistim-search-consoli/index.html#weby-bez-dat",
    "href": "posts/2022-04-09-cistim-search-consoli/index.html#weby-bez-dat",
    "title": "Čístím Search Consoli od zbytečných webů",
    "section": "Weby bez dat",
    "text": "Weby bez dat\nKromě neutorizovaných webů se chci zbavit i těch, které už dlouho nemají žádná data.\nNejdřív si definuju funkci, která načte souhrnné metriky jednoho webu za poslední dva roky. K výsledku přidám do prvního sloupce i URL webu, abych ho poznal, až jich bude v tabulce víc.\n\nget_site_metrics <- function(site) {\n  search_analytics(\n    siteURL = site, \n    startDate = Sys.Date() - 365 * 2, \n  ) |> \n    add_column(site = site, .before = 1)\n}\n\nOvěřím, zda funkce funguje.\n\nget_site_metrics(\"http://www.marekp.cz/\")\n\nFetching search analytics for url: http://www.marekp.cz/ dates: 2021-02-18 2023-02-15 dimensions:  dimensionFilterExp:  searchType: web aggregationType: auto\n\n\n                   site clicks impressions        ctr position\n1 http://www.marekp.cz/   1997       52140 0.03830073 11.22434\n\n\nFunguje, takže ji mohu pomocí funkce map_dfr z balíčku purrr aplikovat na celý seznam webů, ke kterým mám autorizovaný přístup. S více weby to nějakou chvíli poběží.\n\nall_site_metrics <- websites |> \n  filter(permissionLevel != \"siteUnverifiedUser\") |> \n  pull(siteUrl) |> \n  map_dfr(get_site_metrics)\n\nA teď už jen vyfiltruju a zobrazím (opět anonymizovaně) weby, které nemají žádné imprese.\n\nall_site_metrics |> \n  filter(impressions == 0) |> \n  mutate(site = anonymize_site(site))\n\n                      site clicks impressions ctr position\n1   http://anonymized.com/      0           0   0        0\n2   http://anonymized.net/      0           0   0        0\n3   https://anonymized.cz/      0           0   0        0\n4    http://anonymized.sk/      0           0   0        0\n5 sc-domain:anonymized.com      0           0   0        0\n6  https://anonymized.com/      0           0   0        0\n7   https://anonymized.cz/      0           0   0        0\n8   http://anonymized.com/      0           0   0        0\n\n\nPokud je chci ze Search Console odebrat, udělám to podle návodu pro neautorizované weby výše.\nA to je všechno :-)"
  },
  {
    "objectID": "posts/2022-04-09-kontrola-http-kodu/index.html",
    "href": "posts/2022-04-09-kontrola-http-kodu/index.html",
    "title": "Kontrola stavových kódů většího počtu URL v R",
    "section": "",
    "text": "Klient nedávno měnil CMS i doménu webu, došlo ke změně hodně URL, klasický zmatek, jako v těhle případech vždy. Asi měsíc po změně se ukázalo, že se něco nepovedlo a URL starého webu, které měly být přesměrované na nový web, nyní vrací chybu. Navíc jsou některé z nich ještě stále ve výsledcích hledání Googlu.\nVytáhli jsme tedy ze Search Console starého webu URL všech stránek, které se od změny domény alespoň jednou zobrazily, a mým úkolem bylo rychle zkontrolovat, jaké HTTP kódy vracejí. Konkrétně mě zajímalo, která URL fungují (vrací HTTP 200), neexistují (vrací chybu 404 nebo jinou 4xx), nebo na serveru způsobí nějakou chybu (kódy 5xx).\nPro potřeby tohoto zápisku jsem skutečná URL nahradil odkazy z úvodní stránky Wikipedie, ke kterým jsem navíc přidal 10 náhodných adres, aby mi to ukázalo nějaké chyby. Na principu to nic nemění. Ty odkazy jsem získal takhle:\nJedná se o 340 adres, a náhodný vzorek deseti z nich vypadá takhle:"
  },
  {
    "objectID": "posts/2022-04-09-kontrola-http-kodu/index.html#kontrola-jednoho-url",
    "href": "posts/2022-04-09-kontrola-http-kodu/index.html#kontrola-jednoho-url",
    "title": "Kontrola stavových kódů většího počtu URL v R",
    "section": "Kontrola jednoho URL",
    "text": "Kontrola jednoho URL\nPro kontrolu jednoho URL si připravím funkci check_url. Ta zadané URL zkontroluje HTTP požadavkem HEAD (z balíčku httr), zjistí návratový kód a vrátí tibble s původním URL, výsledným URL (z toho se pozná případné přesměrování) a kódem odpovědi. Pro požadavek se také nastaví timeout v sekundách. Pokud server do této doby neodpoví, místo výsledného HTTP kódu se zapíše NA.\n\ncheck_url <- function(url, timeout) {\n  resp <- try(HEAD(url, timeout(timeout)), silent = TRUE)\n  if (class(resp) == \"try-error\") {\n    status <- NA_integer_\n    dest_url <- NA_character_\n  } else {\n    status <- resp$status_code\n    dest_url <- resp$url\n  }\n  tibble(url, dest_url, status)\n}\n\nVyzkouším, zda funkce funguje s platným (ale přesměrovaným) URL.\n\ncheck_url(\"https://wikipedia.org/\", 1)\n\n# A tibble: 1 × 3\n  url                    dest_url                   status\n  <chr>                  <chr>                       <int>\n1 https://wikipedia.org/ https://www.wikipedia.org/    200\n\n\nA raději i s neplatným:\n\ncheck_url(\"https://www.wikipedia.org/iououoiuoiuoiu\", 1)\n\n# A tibble: 1 × 3\n  url                                      dest_url                       status\n  <chr>                                    <chr>                           <int>\n1 https://www.wikipedia.org/iououoiuoiuoiu https://en.wikipedia.org/iouo…    404"
  },
  {
    "objectID": "posts/2022-04-09-kontrola-http-kodu/index.html#kontrola-celého-seznamu-url",
    "href": "posts/2022-04-09-kontrola-http-kodu/index.html#kontrola-celého-seznamu-url",
    "title": "Kontrola stavových kódů většího počtu URL v R",
    "section": "Kontrola celého seznamu URL",
    "text": "Kontrola celého seznamu URL\nA teď již mohu pomocí funkce map_dfr z balíčku purrr zkontrolovat celý seznam URL. Zároveň si budu pomocí funkcí tic a toc z balíčku tictoc měřit, jak dlouho to celé trvá s timeoutem nastaveným na 0.5 sekundy. Reálně by byl potřeba vyšší timeout, např. 3 sekundy, ale Wikiepedia je docela rychlá a já chci ukázat výstup, ve kterém se některá URL v časovém limitu zkontrolovat nepodařilo.\n\ntic()\nstatus_codes <- urls |>\n  map_dfr(check_url, 0.5)\ntoc()\n\n59.27 sec elapsed\n\n\nTrvá to docela dlouho a mohlo by to trvat ještě déle, pokud by byl server pomalejší. Teoreticky až počet URL v seznamu krát timeout. Tak dlouho se mi čekat nechce.\nProto raději zkusím balíček furrr, který nabízí obdobné funkce jako purrr, jenže paralelizované tak, aby využily víc jader a vláken procesoru. Natavím 6 vláken, takže načtení URL by mělo být skoro šestkrát rychlejší.\n\nZrychlení balíčkem furrr\n\nplan(multisession, workers = 6)\n\ntic()\nstatus_codes <- urls |>\n  future_map_dfr(check_url, 0.5)\ntoc()\n\n10.17 sec elapsed\n\n\nJo! Šestkrát rychlejší to sice není, ale i tak je zrychlení super. S tím už se pár tisíc URL zpracovat dá."
  },
  {
    "objectID": "posts/2022-04-09-kontrola-http-kodu/index.html#zobrazení-výsledků",
    "href": "posts/2022-04-09-kontrola-http-kodu/index.html#zobrazení-výsledků",
    "title": "Kontrola stavových kódů většího počtu URL v R",
    "section": "Zobrazení výsledků",
    "text": "Zobrazení výsledků\nA zbývá se podívat na výsledky. Jsou v dataframu (tibble), takže stačí běžné funkce z balíčku dplyr\n\nSouhrnný přehled\n\nstatus_codes |>\n  count(status, sort = TRUE)\n\n# A tibble: 3 × 2\n  status     n\n   <int> <int>\n1    200   329\n2    404    10\n3     NA     1\n\n\n\n\nVadné URL\n\nstatus_codes |> \n  filter(status != 200)\n\n# A tibble: 10 × 3\n   url                                       dest_url                     status\n   <chr>                                     <chr>                         <int>\n 1 https://www.wikipedia.org/HmPsw2WtYSxSgZ6 https://en.wikipedia.org/Hm…    404\n 2 https://www.wikipedia.org/tF2KxtgdzehXaH9 https://en.wikipedia.org/tF…    404\n 3 https://www.wikipedia.org/xtgn1TlDJE8PPM9 https://en.wikipedia.org/xt…    404\n 4 https://www.wikipedia.org/8ESGr2Rn7YC7ktN https://en.wikipedia.org/8E…    404\n 5 https://www.wikipedia.org/f5NHoRoonRkdi0T https://en.wikipedia.org/f5…    404\n 6 https://www.wikipedia.org/DNbL6FfPm6QztsA https://en.wikipedia.org/DN…    404\n 7 https://www.wikipedia.org/8eLeJBm5SVbKUxT https://en.wikipedia.org/8e…    404\n 8 https://www.wikipedia.org/tubP9vI3wi8YxaP https://en.wikipedia.org/tu…    404\n 9 https://www.wikipedia.org/eJJDMz958gctfjW https://en.wikipedia.org/eJ…    404\n10 https://www.wikipedia.org/eomyRJP0BqEE4Fj https://en.wikipedia.org/eo…    404\n\n\n\n\nTimeouty\nA pokud tam jsou i adresy, které nestihly timeout, pak jdou vypsat takhle:\n\nstatus_codes |> \n  filter(is.na(status))\n\n# A tibble: 1 × 3\n  url                                                             dest_…¹ status\n  <chr>                                                           <chr>    <int>\n1 https://itunes.apple.com/app/apple-store/id324715238?pt=208305… <NA>        NA\n# … with abbreviated variable name ¹​dest_url\n\n\nPřípadně je můžu znovu projet s vyšším timeoutem, třeba takhle:\n\nstatus_codes |> \n  filter(is.na(status)) |> \n  pull(url) |> \n  future_map_dfr(check_url, 2)\n\nA to je všechno :-)"
  },
  {
    "objectID": "posts/2022-04-12-search-console-timeline/index.html",
    "href": "posts/2022-04-12-search-console-timeline/index.html",
    "title": "Časové řady z dat Search Console",
    "section": "",
    "text": "Článek je hodně dlouhý, protože sestavit a vizualizovat časové řady (time series) z dat Search Console jde mnoha způsoby a není úplně triviální si vybrat ty, které dávají pro daný účel smysl. Ukážu zde všechny, které bežně používám, i ty, které používám jen výjimečně nebo skoro vůbec.\nKód v jazyce R jsem se snažil napsat tak, aby byl funkční, čistý, srozumitelný a hlavně snadno a univerzálně znovupoužitelný. Časem asi na analýzu dat Search Console udělám samostatný balíček, ale do té doby musí stačit tohle.\nNa druhou stranu má článek i čistě metodickou, na programovacím jazyce nezávislou rovinu. Některé uvedené časové řady a jejich vizualizace jdou vytvořit i jinak, např. v Data Studiu (na příklad odkazuju dál) nebo v Google Sheets. I bez znalosti R jde tedy článek využít jako vzorník výstupů, které jdou z dat Search Console různými prostředky udělat, a které dávají určitý smysl.\nVšechny ukázky čerpají z dat webu Glamour Cabaret Alžběty Faltysové. Jako hobby projekt bez větších investic sice neohromí gigantickými čísly, ale zároveň hezky ukazuje některé vzorce typické pro přirozeně a dobře budovaný obsah se zajímavým potenciálem dalšího růstu. Alžbětě moc děkuju, že mi data pro článek poskytla.\nPro jistotu upozorňuju, že tenhle článek se primárně zabývá konstrukcí a vizualizací časových řad z dat Search Console, nikoli jejich detailní analýzou. Té se budu věnnovat někdy jindy."
  },
  {
    "objectID": "posts/2022-04-12-search-console-timeline/index.html#příprava",
    "href": "posts/2022-04-12-search-console-timeline/index.html#příprava",
    "title": "Časové řady z dat Search Console",
    "section": "Příprava",
    "text": "Příprava\n\nPotřebné balíčky\nJako vždy tidyverse, searchConsoleR pro práci se Search Consolí, lubridate pro práci s daty a časem. Balíček feasts je zde jen pro dekompozici časové řady na trend, sezónnost a šum, a balíček slider umožňuje výpočet klouzavých průměrů. Balíček urltools slouží k rozložení URL na složky a používám ho k segentaci dat podle stránek.\nDlužno dodat, že balíčky specializované na časové řady zde používám jen v nejnutnější míře a zatím mi to tak vyhovuje. Na druhou stranu přiznávám, že některé věci by šly řešit elegantněji plným nasazením specializovaných balíčků typu timetk, tsbox, tsibble apod.\nPoslední řádek nastavuje začátek týdne na pondělí pro balíček lubridate.\n\nlibrary(tidyverse)\nlibrary(searchConsoleR)\nlibrary(lubridate)\nlibrary(feasts)\nlibrary(slider)\nlibrary(urltools)\n\noptions(lubridate.week.start = 1)\n\n\n\nZákladní parametry\nKdykoli pracuju s daty Search Console, nejprve definuju parametry, které určují, jaká vstupní data chci získat. Konkrétně:\n\nsc_site – z kterého webu,\nsc_country – ze které země má hledání pocházet (používám vždy, pokud web primárně cílí na jednu zemi),\ndate_from, date_to – za jaké období data chci.\n\n\nsc_site <- \"https://www.glamourcabaret.cz/\"\nsc_country <- \"cze\"\ndate_from <- as.Date(\"2020-11-01\")\ndate_to <- as.Date(\"2022-04-10\")\n\n\nNastavení časových parametrů balíčkem lubridate\nNěkdy jsem líný hledat v kalendáři vhodná data, nebo chci, aby se mi parametry date_from a date_to automaticky aktualizovaly. Na to se hodí balíček lubridate.\nPro Search Consoli typicky chci datum před třemi dny, protože poslední tři dny ještě data v Search Consoli nejsou finální.\n\ntoday() - 3\n\n[1] \"2023-02-15\"\n\n\nKdyž chci třeba poslední 4 celé kalendářní týdny, udělám to takhle:\n\nperiod_end <- floor_date(today() - 2, unit = \"week\") - 1\nperiod_start <- period_end - weeks(4) + 1\n\ncat(\"Od\", format(period_start, \"%x (%A)\"), \"do\", format(period_end, \"%x (%A)\"))\n\nOd 16.01.2023 (pondělí) do 12.02.2023 (neděle)\n\n\nObdobně můžu určit třeba posledních 6 celých kalendářních měsíců:\n\nperiod_end <- floor_date(today() - 2, unit = \"month\") - 1\nperiod_start <- add_with_rollback(period_end, months(-6), roll_to_first = TRUE)\n\ncat(\"Od\", format(period_start, \"%x\"), \"do\", format(period_end, \"%x\"))\n\nOd 31.07.2022 do 31.01.2023\n\n\nNebo poslední celý rok (ten ale nebude po dubnu běžného roku fungovat, protože Search Console archivuje jen posledních 16 měsíců):\n\nperiod_end <- floor_date(today() - 2, unit = \"year\") - 1\nperiod_start <- floor_date(period_end, \"year\")\n\ncat(\"Od\", format(period_start, \"%x\"), \"do\", format(period_end, \"%x\"))\n\nOd 01.01.2022 do 31.12.2022\n\n\n\n\n\nNačtení vstupních dat\nPak si podle potřeby definuju funkci, která při prvním zavolání s konkrétními parametry načte data funkcí search_analytics z balíčku searchConsoleR a uloží je do souboru. Při druhém a dalším zavolání se stejnými parametry již načítá data z uloženého souboru. To má dvě výhody:\n\nOpakované načtení dat je mnohem rychlejší.\nSkript pokaždé pracuje se stejnými daty, i když už ze Search Console zmizela (jsou tam data jen za posledních 16 měsíců).\n\nA dvě nevýhody:\n\nUložená data zabírají místo na disku.\nPokud potřebuju čerstvá data nebo s jinými parametry, musím soubory z disku odstranit.\n\nPro účely tohoto zápisku si funkci definuju velmi jednoduše. Jako jediný parametr má dimenze, vše ostatní se bere přímo z globálních objektů definovaných výše. Data ukládá do podsložky raw-data, která již v aktuální složce musí existovat. Pro jiné účely může být funkce složitější.\n\nread_sc <- function(dimensions) {\n  rds_path <- file.path(\n    \".\", \"data-raw\", paste0(\"sc-\", paste(dimensions, collapse = \"-\"), \".rds\")\n  )\n  row_limit <- as.integer(date_to - date_from) * 500 * (length(dimensions))\n\n  if (!file.exists(rds_path)) {\n    search_analytics(\n      siteURL = sc_site,\n      startDate = date_from,\n      endDate = date_to,\n      dimensions = dimensions,\n      dimensionFilterExp = paste0(\"country==\", sc_country),\n      rowLimit = row_limit\n    ) |>\n      write_rds(rds_path, compress = \"gz\")\n  }\n  read_rds(rds_path)\n}\n\nNakonec si opravdu stáhnu resp. ze souborů načtu všechna data, která budu ze Search Console potřebovat. V praxi se s tím musí opatrně, protože tahání více dimenzí za delší období z API může trvat docela dlouho. Když nevím, jak budou data konkrétního webu veliká, zkusím nejprve stáhnout jeden typický den nebo týden a podle toho se rozhodnu.\nPokud budete kód sami pouštět, funkci src_auth můžete nechat bez parametrů a ona se vás na e-mail zeptá. Já to mám takhle proto, aby to běželo automaticky.\n\nscr_auth(email = Sys.getenv(\"MY_GOOGLE_ACCOUNT\"))\n\n✔ Setting client.id from C:/Users/HP Z1/AppData/Local/R/win-library/4.2/searchConsoleR/clients/native.json\n\nsc_by_date <- read_sc(\"date\")\nsc_by_date_page <- read_sc(c(\"date\", \"page\"))\nsc_by_date_query <- read_sc(c(\"date\", \"query\"))\nsc_by_date_page_query <- read_sc(c(\"date\", \"page\", \"query\"))\n\nVýsledkem je několik dataframů, jejichž strukturu a obsah si mohu rychle zkontrolovat.\n\nglimpse(sc_by_date)\n\nRows: 498\nColumns: 5\n$ date        <date> 2020-11-28, 2020-11-29, 2020-11-30, 2020-12-01, 2020-12-0…\n$ clicks      <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ impressions <int> 0, 0, 0, 4, 5, 1, 0, 0, 1, 0, 2, 10, 4, 6, 3, 6, 6, 2, 6, …\n$ ctr         <dbl> 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00…\n$ position    <dbl> 0.000000, 0.000000, 0.000000, 16.500000, 8.400000, 11.0000…\n\n\n\nglimpse(sc_by_date_page)\n\nRows: 18,626\nColumns: 6\n$ date        <date> 2021-12-19, 2021-07-02, 2021-04-19, 2021-07-05, 2021-06-3…\n$ page        <chr> \"https://www.glamourcabaret.cz/znacky/hermes\", \"https://ww…\n$ clicks      <int> 30, 13, 12, 12, 11, 11, 10, 8, 8, 8, 8, 8, 7, 7, 7, 7, 7, …\n$ impressions <int> 657, 272, 406, 100, 374, 768, 295, 91, 226, 105, 141, 105,…\n$ ctr         <dbl> 0.04566210, 0.04779412, 0.02955665, 0.12000000, 0.02941176…\n$ position    <dbl> 3.713851, 3.669118, 5.211823, 3.470000, 4.687166, 3.281250…\n\n\nA tak dále. Výsledný dataframe vždy obsahuje dimenzi date, případné další dimenze a čtyři základní metriky, které Search Console eviduje.\n\n\nNekonzistence dat Search Console\nTeoreticky by stačilo stáhnout jenom poslední dataset sc_by_date_page_query, protože z něho by mělo jít všechno ostatní agregovat, jenže Search Console vrací různě upravené výsledky podle toho, zda si vyžádáte dimenze page nebo query, takže datasety bez těchto dimenzí mají přesnější čísla než data sety s nimi. Podrobněji to dokumentuju v tomhle testu.\nZ tohoto důvodu beru základní metriky (imprese, kliky, CTR a pozice) pokud možno jen z datasetu, který nebosahuje dimenze page nebo query. Datasety s těmito dimenzemi používám jen v případě, že potřebuju znát konkrétní stránky nebo jejich počty, resp. konkrétní dotazy nebo jejich počty."
  },
  {
    "objectID": "posts/2022-04-12-search-console-timeline/index.html#vizualizace-časových-řad",
    "href": "posts/2022-04-12-search-console-timeline/index.html#vizualizace-časových-řad",
    "title": "Časové řady z dat Search Console",
    "section": "Vizualizace časových řad",
    "text": "Vizualizace časových řad\n\nZákladní metriky\nZákladní vizualizace, kterou skoro stejně ukazuje i webové rozhraní Search Console, vypadá takto:\n\nscale_factor <- max(sc_by_date$clicks) / max(sc_by_date$impressions)\nsc_by_date |>\n  ggplot(aes(x = date)) +\n  geom_line(aes(y = clicks), color = \"#4285f4\") +\n  geom_line(aes(y = impressions * scale_factor), color = \"#5e35b1\") +\n  scale_y_continuous(\n    name = \"clicks\",\n    sec.axis = sec_axis(~ . / scale_factor, name = \"impressions\")\n  ) +\n  theme(\n    axis.title.y.left = element_text(color = \"#4285f4\"),\n    axis.text.y.left = element_text(color = \"#4285f4\"),\n    axis.title.y.right = element_text(color = \"#5e35b1\"),\n    axis.text.y.right = element_text(color = \"#5e35b1\")\n  ) +\n  theme_minimal()\n\n\n\n\nTenhle graf má duální osu Y (čili dvě různé osy Y), což patří spíš do rejstříku „manažerských“ grafů, které se ve vážnější analytice z dobrých důvodů moc nepoužívájí. V R se místo toho většinou používají fasety (grafy vedle sebe a/nebo pod sebou), které nezkreslují měřítko, nepletou čtenáře co je co a navíc se do nich vejdou víc než dvě metriky, když je to potřeba.\nProtože budu v dalším kódu potřebovat podobné fasetové grafy často, připravím si na ně dvě funkce, abych nepsal opakovaně stejný kód.\nPrvní funkce sc_reshape provede tři věci:\n\nPřesune imprese před kliky, protože tohle pořadí mi připadá logičtější.\nPřevede širokou tabulku na dlouhou, tj. hodnoty metrik v druhém až posledním sloupci sloučí do jediného sloupce value (to je ve funkci pivot_longer default) a jejich názvy do sloupce metric.\nPřevede názvy metrik na typ factor, aby se fasety v grafu správně seřadily.\n\nK druhému bodu vysvětlení: Fasety v principu zobrazují jen jednu proměnnou ve více kategoriích, takže se tabulka musí předem převést na dlouhý formát, ve kterém se do jednoho sloupce dostanou názvy metrik jako kategorie a do druhého sloupce hodnoty všech metrik. K tomu slouží funkce pivot_longer.\n\nsc_reshape <- function(df) {\n  if (\"impressions\" %in% names(df)) {\n    df <- df |> \n      relocate(impressions, .before = clicks)\n  }\n  df |>\n    pivot_longer(cols = 2:last_col(), names_to = \"metric\") |>\n    mutate(metric = as_factor(metric))\n}\n\nDruhá funkce vykreslí samotný graf časových řad. Protože budu používat různé časové rozpětí a různé časové jednotky, je malinko složitější, aby se tomu přizpůsobila.\n\nsc_plot <- function(\n  df, x_breaks = \"3 months\", x_labels = \"%b %y\", x_minor_breaks = \"month\",\n  strip_pos = \"right\"\n) {\n  df |> \n    ggplot(aes(x = .data[[names(df)[[1]]]], y = value)) +\n    geom_line() +\n    scale_x_date(\n      date_breaks = x_breaks, date_labels = x_labels,\n      date_minor_breaks = x_minor_breaks, expand = expansion(0.01)\n    ) +\n    facet_wrap(\n      ~ metric, ncol = 1, scales = \"free_y\", strip.position = strip_pos\n    ) +\n    theme_gray()\n}\n\nVe fasetách vypadají časové řady všech čtyř základních metrik takhle:\n\nsc_by_date |>\n  sc_reshape() |>\n  sc_plot()\n\n\n\n\n\nVýklad dat\nUž tenhle graf jde nějak interpretovat:\n\nNa začátku období neměl web z Googlu skoro žádnou návštěvnost, ale po cca třech měsících se začal postupně zlepšovat a zřejmě se zlepšuje pořád.\nCTR v prvním měsíci skákalo až k 50 %, což s nepatrným počtem impresí naznačuje, že web se v té době zobrazoval jen na extrémně nekonkurenční dotazy, které neměly ve výsledcích hledání jinou dobrou odpověď. To se postupně změnilo, CTR kleslo, a to naznačuje, že přibyly nějaké normálnější dotazy.\nTuto hypotézu podporuje i křivka průměrných pozic. Na začátku šíleně skáče, což je typické pro velmi malý počet dotazů, později se uklidní, takže se počet dotazů asi zvýšil.\nZ toho všecho bych usoudil, že web začínal s malým objemem obsahu a postupně se zvětšuje. Je to ale jen hypotéza, kterou je třeba podpořit dalšími daty.\n\n\n\n\nPočet stránek a dotazů\nAby šla data lépe interpretovat, skoro vždy přidávám ještě dvě další časové řady – počet vstupních stránek a počet unikátních dotazů. K tomu jsem si stáhl ze Search Sonsole dva další datasety, vstupní stránky podle dnů (sc_by_date_page) dotazy podle dnů (sc_by_date_query). Nyní z nich funkcemi group_by a summarise spočítám počty unikátních stránek resp. dotazů za den a funkcí left_join je připojím k původní tabulce metrik. Výsledek uložím do objektu sc_all_metrics_by_date.\n\nsc_all_metrics_by_date <- sc_by_date |>\n  left_join(\n    sc_by_date_page |>\n      group_by(date) |>\n      summarise(pages = n()),\n    by = \"date\"\n  ) |>\n  left_join(\n    sc_by_date_query |>\n      group_by(date) |>\n      summarise(queries = n()),\n    by = \"date\"\n  ) |>\n  mutate(\n    pages = replace_na(pages, 0),\n    queries = replace_na(queries, 0)\n  )\n\nVýslední tabulka vypadá takhle:\n\nglimpse(sc_all_metrics_by_date)\n\nRows: 498\nColumns: 7\n$ date        <date> 2020-11-28, 2020-11-29, 2020-11-30, 2020-12-01, 2020-12-0…\n$ clicks      <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ impressions <int> 0, 0, 0, 4, 5, 1, 0, 0, 1, 0, 2, 10, 4, 6, 3, 6, 6, 2, 6, …\n$ ctr         <dbl> 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00…\n$ position    <dbl> 0.000000, 0.000000, 0.000000, 16.500000, 8.400000, 11.0000…\n$ pages       <int> 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 0, 1, 1…\n$ queries     <int> 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 2, 1, 2, 2, 1, 1, 0, 1, 1…\n\n\nGraf zobrazím stejně, jako ten se čtyřmi základními metrikami, jen funkcí scale_x_date trochu vyladím rozdělení osy X, aby bylo časové členění přehlednější.\n\nsc_all_metrics_by_date |>\n  sc_reshape() |>\n  sc_plot()\n\n\n\n\nMimochodem, obdobný graf jde odělat i v Data Studiu. Tady je příklad reportu, který si můžete zkopírovat a upravit pro své potřeby.\n\n\nZoomování grafu\nV druhé polovině grafu za celé období nejsou moc dobře vidět změny CTR a pozic, protože na začátku období mají tyto metriky příliš veliké rozpětí hodnot. Zazoomuju proto na období od začátku srpna 2021 prostým vyfiltrováním řádků, které do grafu vstupují.\n\nsc_all_metrics_by_date |>\n  filter(date >= as.Date(\"2021-08-01\")) |>\n  sc_reshape() |>\n  sc_plot(\"month\")"
  },
  {
    "objectID": "posts/2022-04-12-search-console-timeline/index.html#granularita-období",
    "href": "posts/2022-04-12-search-console-timeline/index.html#granularita-období",
    "title": "Časové řady z dat Search Console",
    "section": "Granularita období",
    "text": "Granularita období\nAž dosud jsem časové řady vykresloval po dnech. Pro data související s návštěvností webů je ale typická silná týdenní periodicita a v jejím důsledku grafy bývají hodně zubaté. Existuje několik způsobů, jak je vyhladit. Nejpřirozenější je snížit časovou granularitu a agregovat data za týden, měsíc, nebo ještě delší období. I to lze však udělat více způsoby, např mohu:\n\nSečíst to, co jde sečíst (imprese a kliky) a zprůměrovat to, co sčítat nejde (CTR a pozice) a počty unikátních položek (stránek a dotazů) spočítat znovu. To vše za celé nové časové jednotky, tedy např. za týdny.\nTotéž, ale klouzavě. Ke každému dni tedy totéž sečtu, zprůměruju, nebo znovu vypočtu za posledních sedm dní. Bude to sice výpočetně náročnější, protože místo jednou za týden to budu počítat za každý den, ale i když nebude celé období začínat a končit přesně na hranici týdnů (resp. měsíců či jiné časové jednotky), nebude graf na okrajích zkreslený.\nSpočítat prosté průměry či mediány všech hodnot za týden či jinou časovou jednotku.\nKromě průměru či mediánu spočítat a hezky vykreslit i pásmo, ve kterém se hodnoty pohybují.\n\nZkusím všechny varianty, ať je vidět, zda a případně jak se liší.\n\nAgregace za kalendářní období\nAgregace dat Search Console do delších časových jednotek, tedy typicky týdnů či měsíců případně kvartálů, není úplně triviální.\n\nImprese a kliky se normálně sečtou, tam problém není.\nCTR a pozice se musí zprůměrovat váženým průměrem, přičemž vahou jsou imprese. Není to úplně nezbytné, ale je to bezpečnější, protože prostý aritmetický průměr se od váženého někdy liší jen nepatrně a jindy (při velkém rozdílu impresí) hodně.\nPočty unikátních stránek a dotazů se musí spočítat funkcí n_distinct za každé dílčí období znovu. Rozdíl ve tvaru křivky sice nebývá moc veliký, ale zato je značný rozdíl ve velikosti hodnot – za den je unikátních dotazů či stránek podstatně méně než za týden nebo měsíc.\n\nPro usnadnění a zpřehlednění výpočtu si připravím několik funkcí.\nPrvní funkce agreguje tabulku základních metrik.\n\nsc_aggregate <- function(df, per = \"week\") {\n  df |>\n    group_by(\"{per}\" := floor_date(date, per)) |> \n    summarise(\n      across(ctr:position, ~ weighted.mean(.x, impressions)),\n      across(clicks:impressions, sum),\n    ) |> \n    relocate(ctr:position, .after = impressions)\n}\n\nsc_aggregate(sc_by_date, \"quarter\")\n\n# A tibble: 7 × 5\n  quarter    clicks impressions    ctr position\n  <date>      <int>       <int>  <dbl>    <dbl>\n1 2020-10-01      4          89 0.0449     9.19\n2 2021-01-01     80        2608 0.0307     8.86\n3 2021-04-01    464       16597 0.0280     6.79\n4 2021-07-01   1224       54718 0.0224     7.35\n5 2021-10-01   2160      112025 0.0193     7.18\n6 2022-01-01   2816      142337 0.0198     6.82\n7 2022-04-01    357       17905 0.0199     6.99\n\n\nDruhá funkce znovu spočítá počty unikátních stránek či dotazů. Předpokládá, že vstupní tabulka obsahuje sloupce date a sloupec řetězců, keré se mají spočítat. Na dalších sloupcích nezáleží.\n\nsc_recount <- function(df, per = \"month\", column, new_col) {\n  df |> \n    group_by(\"{per}\" := floor_date(date, per)) |> \n    summarise(\"{new_col}\" := n_distinct({{ column }}))\n}\n\nsc_recount(sc_by_date_page, \"quarter\", page, \"pages\")\n\n# A tibble: 7 × 2\n  quarter    pages\n  <date>     <int>\n1 2020-10-01     3\n2 2021-01-01    33\n3 2021-04-01    78\n4 2021-07-01   126\n5 2021-10-01   141\n6 2022-01-01   150\n7 2022-04-01   126\n\n\nA konečně mohu napsat poslední funkci, která sestaví všechny metriky do jedné tabulky.\n\nsc_aggregate_all_metrics <- function(per = \"week\") {\n  sc_aggregate(sc_by_date, per) |> \n    left_join(sc_recount(sc_by_date_page, per, page, \"pages\")) |> \n    left_join(sc_recount(sc_by_date_query, per, query, \"queries\")) |> \n    mutate(\n      pages = replace_na(pages, 0),\n      queries = replace_na(queries, 0)\n    )\n}\n\nsc_aggregate_all_metrics(\"quarter\")\n\n# A tibble: 7 × 7\n  quarter    clicks impressions    ctr position pages queries\n  <date>      <int>       <int>  <dbl>    <dbl> <int>   <int>\n1 2020-10-01      4          89 0.0449     9.19     3       5\n2 2021-01-01     80        2608 0.0307     8.86    33      71\n3 2021-04-01    464       16597 0.0280     6.79    78     345\n4 2021-07-01   1224       54718 0.0224     7.35   126    1158\n5 2021-10-01   2160      112025 0.0193     7.18   141    1923\n6 2022-01-01   2816      142337 0.0198     6.82   150    2369\n7 2022-04-01    357       17905 0.0199     6.99   126     868\n\n\n\nAgregace po kalendářních týdnech\nS připravenými funkcemi již mohu snadno agregovat.\n\nsc_aggregate_all_metrics(\"week\") |> \n  filter(week >= floor_date(as.Date(\"2021-08-01\"), \"week\")) |>\n  sc_reshape() |>\n  sc_plot(\"4 weeks\", \"%x\", \"week\")\n\n\n\n\n\n\nAgregace po kalendářních měsících\n\nsc_aggregate_all_metrics(\"month\") |> \n  filter(\n    month >= floor_date(as.Date(\"2021-08-01\"), \"month\"),\n    month <= floor_date(as.Date(\"2022-03-01\"), \"month\")\n  ) |>\n  sc_reshape() |>\n  sc_plot(\"month\")\n\n\n\n\n\n\n\nPrůměr za kalendářní týdny\nPrůměr za kalendářní týdny je výpočetně mnohem jednodušší. Stačí spočítat průměry všech číselných hodnot. Jak ale píšu výše, pro CTR a pozice je metodicky nesprávný, a proto někdy méně a jindy více nepřesný, a u počtů stránek a dotazů sice metodicky dává smysl, nicméně říká něco jiného.\nKdyž si porovnáte tvar křivek metrik pages a queies, jsou sice podobné, ale čísla jsou úplně jiná – v prvním výpočtu (počet za týden) jsem se dostal na 110 stránek a 800 dotazů, kdežto v druhém (průměr týdenního počtu) jen na 80 resp. 300.\n\nsc_all_metrics_by_date |>\n  mutate(week = floor_date(date, \"week\")) |> \n  filter(week >= floor_date(as.Date(\"2021-08-01\"), \"week\")) |>\n  group_by(week) |> \n  summarise(across(clicks:queries, mean)) |> \n  sc_reshape() |> \n  sc_plot(\"4 weeks\", \"%x\", \"week\")\n\n\n\n\n\n\nRozsah hodnot za týden\nNa tomto grafu vykreslím medián jako čáru, mezikvartilové rozpětí (interquartile range, tj. rozsah mezi 1. a 3. kvartilem) jako tmavší pás a variační rozpětí (rozdíl mezi maximem a minimem) jako světlejší pás.\n\nsc_all_metrics_by_date |>\n  mutate(week = floor_date(date, \"week\"), .before = date) |> \n  filter(week >= floor_date(as.Date(\"2021-08-01\"), \"week\")) |> \n  select(!date) |> \n  sc_reshape() |> \n  group_by(week, metric) |> \n  summarise(\n    median = median(value),\n    lower = quantile(value, probs = 0.25),\n    upper = quantile(value, probs = 0.75),\n    min = min(value),\n    max = max(value),\n    .groups = \"drop\"\n  ) |> \n  ggplot(aes(x = week)) +\n  geom_ribbon(aes(ymin = min, ymax = max), fill = \"gray80\") +\n  geom_ribbon(aes(ymin = lower, ymax = upper), fill = \"gray60\") +\n  geom_line(aes(y = median), color = \"gray25\") +\n  scale_x_date(\n    date_breaks = \"4 weeks\", date_labels = \"%x\", date_minor_breaks = \"week\",\n    expand = expansion(0.025)\n  ) +\n  facet_wrap(~metric, ncol = 1, scales = \"free_y\", strip.position = \"right\")\n\n\n\n\n\n\nKlouzavý průměr (moving average)\nKlouzavý průměr za posledních sedm dní zpopularizovaly různé covidové statistiky, ale pro data Search Console obvykle nemá smysl. Hodí se jen v případě, že potřebujete znát co nejnovější hodnotu (typicky včerejší) očištěnou od týdenní sezónnosti, přestože týden ještě neskončil.\nPro jistotu ukážu, jak jde klouzavý průměr za posledních 7 dní spočítat (zde jen imprese), přestože ho sám nepoužívám. Pod něj zároveň šedou barvou vykreslím skutečné imprese po dnech.\n\nsc_by_date |>\n  mutate(\n    impressions_avg7 = slider::slide_dbl(impressions, ~ mean(.x), .before = 6)\n  ) |>\n  ggplot(aes(x = date)) +\n  geom_line(aes(y = impressions), color = \"gray\") +\n  geom_line(aes(y = impressions_avg7)) +\n  scale_x_date(\n    date_breaks = \"3 months\", date_labels = \"%b %y\", \n    date_minor_breaks = \"month\", expand = expansion(0.01)\n  )\n\n\n\n\n\n\nRegrese\nV praxi je úplně nejjednodušší vyhledit denní křivku nějakou regresí. V balíčku ggplot2 je na to hotový geom_smooth.\n\nsc_by_date |>\n  sc_reshape() |>\n  sc_plot() +\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nPokud chci křivku klikatější, jde to nastavit parametrem span.\n\nsc_by_date |>\n  sc_reshape() |>\n  sc_plot() +\n  geom_smooth(span = 0.1)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nA ještě ukážu, jak jde geom_smooth vykreslit na jediné metrice za kratší období, protože to je typ grafu, který asi používám v různých reportech pro klienty nejčastěji. Šedý pás okolo modré křivky je konfidenční interval.\n\nsc_by_date |>\n  filter(date >= as.Date(\"2022-01-01\")) |> \n  ggplot(aes(x = date, y = impressions)) +\n  geom_line(color = \"gray50\") +\n  geom_smooth() +\n  scale_x_date(date_labels = \"%b %y\")\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'"
  },
  {
    "objectID": "posts/2022-04-12-search-console-timeline/index.html#poměrové-metriky",
    "href": "posts/2022-04-12-search-console-timeline/index.html#poměrové-metriky",
    "title": "Časové řady z dat Search Console",
    "section": "Poměrové metriky",
    "text": "Poměrové metriky\nZ předešlých grafů už vím, že imprese i kliky dlouhodobě rostou a že to je mj. dáno tím, že roste počet vstupních stránek a dotazů. Tahle informace mi ale nestačí. Existují totiž tři možné varianty:\n\nImprese a kliky rostou víc, než by odpovídalo nárůstu počtu stránek a dotazů. To by se mi líbilo nejvíc, protože by to znamenalo, že novější stránky resp. dotazy jsou úspěšnější než starší, případně se na růstu impresí a kliků podílí ještě něco jiného, např. celková síla webu.\nRúst impresí a kliků zhruba odpovídá růstu počtu stránek a dotazů. To je neutrální výsledek – odměna za vynaloženou práci na obsahu (a tedy efektivita této práce) zůstává pořád zhruba stejná, ani neklesá, ani se nezvyšuje.\nPoslední varianta je nejhorší: imprese a kliky rostou méně, než by odpovídalo nárůstu počtu stránek a dotazů. Efektivita vložené práce se tedy zhoršuje. Někdy to jinak nejde, protože nízko visíci ovoce je už otrhané, ale je dobré o tom vědět.\n\nNa tyhle otázky pomáhají najít odpověď poměrové metriky. V principu jde o poměry počtu impresí a kliků na jedné straně a počtu stránek a dotazů na straně druhé. Navíc je zajímavý i poměr počtu dotazů na stránku.\n\nsc_aggregate_all_metrics() |> \n  filter(week >= floor_date(as.Date(\"2021-08-01\"), \"week\")) |> \n  transmute(\n    week = week,\n    impressions_per_page = impressions / pages,\n    clicks_per_page = clicks / pages,\n    impressions_per_query = impressions / queries,\n    clicks_per_query = clicks / queries,\n    queries_per_page = queries / pages\n  ) |>\n  sc_reshape() |> \n  sc_plot(\"4 weeks\", \"%x\", \"week\", strip_pos = \"top\")\n\n\n\n\nZ grafu je vidět, že počty impresí a kliků na stránku stále rostou, což je dobře – odpovídá to variantě 1. S impresemi a kliky na dotaz je to už slabší, ty posledních cca 5 měsíců stagnují. Co to znamená? Na to odpovídá poslední poměrová metrika, počet dotazů na stránku. Ta roste, což znamená, že novější stránky jsou efektivnější především proto, že pokrývají víc dotazů, avšak jednotlivé novéjší dotazy jsou efektivní zhruba stejně.\nK tomu jen poznámka: Grafy samy o sobě nejsou důkazem tvrzení, která zde píšu. Jsou jen dobrým podkladem k odbornému odhadu, který je tím spolehlivější, čím lépe znám daný web a jeho historii."
  },
  {
    "objectID": "posts/2022-04-12-search-console-timeline/index.html#metriky-podle-pozic",
    "href": "posts/2022-04-12-search-console-timeline/index.html#metriky-podle-pozic",
    "title": "Časové řady z dat Search Console",
    "section": "Metriky podle pozic",
    "text": "Metriky podle pozic\nOblíbené vizualizace jsou různé metriky podle pozic nebo intervalů pozic. Např. Collabim a Marketing Miner nabízí graf počtu dotazů podle skupin pozic. I takové grafy jsou ze Search Console udělat.\n\nPočet dotazů podle intervalů pozic\nV Collabimu a Marketing Mineru je 100% plošný graf počtu dotazů podle intervalů pozic. Ten v principu vypadá takhle, byť má intervaly pozic nastavené jinak:\n\nsc_by_date_query |>\n  filter(date >= as.Date(\"2021-08-01\")) |> \n  mutate(\n    pos_int = cut(\n      position,\n      breaks = c(10 * 0:3, Inf),\n      labels = c(\"1-10\", \"11-20\", \"21-30\", \"31+\")\n    )\n  ) |>\n  group_by(date, pos_int) |>\n  summarise(queries = n_distinct(query)) |>\n  ungroup() |> \n  ggplot(aes(x = date, y = queries, fill = pos_int)) +\n  geom_area(position = \"fill\") +\n  scale_x_date(\n    date_breaks = \"3 months\", date_labels = \"%b %y\", \n    date_minor_breaks = \"month\", expand = expansion(0.01)\n  ) +\n  scale_fill_brewer(type = \"seq\", direction = -1)\n\n\n\n\nTenhle graf má ale význam jen pro pevně danou sadu dotazů, která se v čase nemění. V kontextu Search Console, kde se dotazy a jejich počet každý den mění, nedává příliš smysl. O trochu užitečnější je graf skutečných počtů dotazů nepřepočítaných na procenta. Z toho je alespoň vidět, že počet dotazů roste a růst táhnou dotazy v první desítce.\n\nsc_by_date_query |>\n  filter(date >= as.Date(\"2021-08-01\")) |> \n  mutate(\n    pos_int = cut(\n      position,\n      breaks = c(10 * 0:3, Inf),\n      labels = c(\"1-10\", \"11-20\", \"21-30\", \"31+\")\n    )\n  ) |>\n  group_by(date, pos_int) |>\n  summarise(queries = n_distinct(query), .groups = \"drop\") |>\n  ggplot(aes(x = date, y = queries, fill = pos_int)) +\n  geom_area() +\n  scale_x_date(\n    date_breaks = \"3 months\", date_labels = \"%b %y\", \n    date_minor_breaks = \"month\", expand = expansion(0.01)\n  ) +\n  scale_fill_brewer(type = \"seq\", direction = -1)\n\n\n\n\n\n\nPočet vstupních stránek podle intervalů pozic\nObdobný graf lze vykreslit i pro počty vstupních stránek\n\nsc_by_date_page |>\n  filter(date >= as.Date(\"2021-08-01\")) |> \n  mutate(\n    pos_int = cut(\n      position,\n      breaks = c(10 * 0:3, Inf),\n      labels = c(\"1-10\", \"11-20\", \"21-30\", \"31+\")\n    )\n  ) |>\n  group_by(date, pos_int) |>\n  summarise(pages = n_distinct(page), .groups = \"drop\") |>\n  ggplot(aes(x = date, y = pages, fill = pos_int)) +\n  geom_area() +\n  scale_x_date(\n    date_breaks = \"3 months\", date_labels = \"%b %y\", \n    date_minor_breaks = \"month\", expand = expansion(0.01)\n  ) +\n  scale_fill_brewer(type = \"seq\", direction = -1)\n\n\n\n\n\n\nImprese podle intervalů pozic\nGraf jde vykreslit i pro imprese nebo kliky (zde imprese). Zároveň jsem mu nastavil i jiné intervaly pozic, aby bylo vidět pordobnější rozdělení pozic na první stránce výsledků hledání.\nAle upřímně, všechny tyhle grafy mají z dat Search Console málokdy smysl, protože bývají pro většinu webů hodně podobné a nějaký důležitý vhled z nich získám jen výjimečně.\n\nsc_by_date_query |>\n  filter(date >= as.Date(\"2021-08-01\")) |> \n  mutate(\n    pos_int = cut(\n      position,\n      breaks = c(1, 1.5, 3.5, 5.5, 10.01, Inf),\n      labels = c(\"1\", \"2-3\", \"4-5\", \"6-10\", \"10+\"),\n      right = FALSE\n    )\n  ) |>\n  group_by(date, pos_int) |>\n  summarise(impressions = sum(impressions), .groups = \"drop\") |>\n  ggplot(aes(x = date, y = impressions, fill = pos_int)) +\n  geom_area() +\n  scale_x_date(\n    date_breaks = \"3 months\", date_labels = \"%b %y\", \n    date_minor_breaks = \"month\", expand = expansion(0.01)\n  ) +\n  scale_fill_brewer(type = \"seq\", direction = -1)"
  },
  {
    "objectID": "posts/2022-04-12-search-console-timeline/index.html#porovnání-metrik-s-předešlým-obdobím-nebo-rokem",
    "href": "posts/2022-04-12-search-console-timeline/index.html#porovnání-metrik-s-předešlým-obdobím-nebo-rokem",
    "title": "Časové řady z dat Search Console",
    "section": "Porovnání metrik s předešlým obdobím nebo rokem",
    "text": "Porovnání metrik s předešlým obdobím nebo rokem\nDalším populárním grafem je porovnání běžného období s obdobím před rokem (YoY, year over year) nebo s obdobím bezprostředně předcházejícím.\n\nMeziroční porovnání (YoY)\nPrincip přípravy je jednoduchý. Vím, že mám v dlouhé tabulce 6 metrik, takže ke každému řádku přidám funkcí lag hodnotu z řádku o 6 * 365 řádků zpátky. Tuhle hodnotu pak vykreslím tečkovanou čárou.\n\nsc_all_metrics_by_date |> \n  sc_reshape() |> \n  mutate(value_prev = lag(value, 6 * 365)) |> \n  filter(date >= as.Date(\"2022-01-01\")) |>\n  sc_plot(\"month\") +\n  geom_line(aes(y = value_prev), linetype = \"dotted\")\n\n\n\n\n\n\nPosledních 12 týdnů a 12 týdnů před tím\nObdobně jde vykreslit jakékoli období s jakoukoli granularitou a k němu stejně dlouhé období bezprostředně předcházející.\n\nsc_aggregate_all_metrics(\"week\") |> \n  sc_reshape() |> \n  mutate(value_prev = lag(value, 6 * 12)) |> \n  filter(week >= floor_date(as.Date(\"2022-01-03\"), \"week\")) |> \n  sc_plot(\"2 weeks\", \"%x\", \"week\") +\n  geom_line(aes(y = value_prev), linetype = \"dotted\")\n\n\n\n\n\n\nAutokorelace\nZajímavá varianta je, když se vykreslí jen rozdíly (případně podíly resp. procenta) mezi aktuální a předešlou hodnotou. V principu se jedná o svislou úsečku mezi plnou a tečkovanou z grafů výše, která má základ v nule.\nZde je příklad posledních 12 týdnů v porovnání s předešlými 12 týdny vykreslený po dnech. Ukazuje, o kolik byla daná metrika v daný den vyšší než před 12 týdny.\n\nsc_all_metrics_by_date |>\n  sc_reshape() |> \n  mutate(value = value - lag(value, 6 * 7 * 12)) |> \n  filter(date >= as.Date(\"2022-01-01\")) |>\n  ggplot(aes(x = date, y = value)) +\n  geom_segment(aes(xend = date, yend = 0)) +\n  scale_x_date(date_breaks = \"month\", date_labels = \"%b %y\") +\n  facet_wrap(~metric, ncol = 1, scales = \"free_y\", strip.position = \"right\")"
  },
  {
    "objectID": "posts/2022-04-12-search-console-timeline/index.html#dny-v-týdnu",
    "href": "posts/2022-04-12-search-console-timeline/index.html#dny-v-týdnu",
    "title": "Časové řady z dat Search Console",
    "section": "Dny v týdnu",
    "text": "Dny v týdnu\nPro většinu webů je typická týdenní periodicita. Někde je slabší, někde silnější a liší se i její průběh. Asi nejlepším porovnáním je boxplot, který ukazuje jednak medián (tlustší vodorovná čára uvnitř boxu) a jednak i celou distribuci hodnt (box reprezentuje 50 % hodnot uprostřed, box i s fousy rozpětí bez odlehlých hodnot, které reprezentují puntíky).\nGraf má smysl spíš jen pro imprese a kliky, protože u ostatních metrik se periodicita moc neprojevuje.\n\nsc_by_date |>\n  filter(date >= as.Date(\"2021-08-01\")) |>\n  select(!c(ctr, position)) |> \n  sc_reshape() |> \n  mutate(day = wday(date, label = TRUE, week_start = 1)) |> \n  ggplot(aes(x = day, y = value)) +\n  geom_boxplot() +\n  facet_wrap(~metric, ncol = 1, scales = \"free_y\", strip.position = \"right\")\n\n\n\n\nZ grafu je vidět, že zrovna tenhle web silnou týdenní periodicitu nemá. Sice od pondělí trochu vystoupá do úterý, pak až do soboty mírně klesá a znovu stoupne v neděli, ale rozdíly hodnot nejsou moc přesvědčivé.\n\nDekompozice časové řady\nPro dekompozici časových řad na trend, sezónnost a šum existuje mnoho teoretických modelů a balíčků, které je počítají. Pro příklad ukážu postup s balíčkem feasts z rodiny balíčku tsibble. Ten sice nějakou sezónnost našel, ale zároveň ukazuje, že se pohybuje jen v rozpětí cca 100 impresí (-50 až 50), což v objemu nějakých 2000 impresí za den není zrovna moc. Navíc je vidět, že rozpětí šumu (označený jako random) je čtyřikrát větší.\n\nsc_by_date |>\n  filter(date >= as.Date(\"2022-01-01\")) |> \n  as_tsibble(index = date) |>\n  fabletools::model(feasts::classical_decomposition(impressions)) |>\n  fabletools::components() |>\n  autoplot()\n\nWarning: Removed 3 rows containing missing values (`geom_line()`).\n\n\n\n\n\nNutno poznamenat, že sezónnost v v rámci roku nejde z dat Search Console odvodit, protože je k dispozici jen 16 měsíců dat. Museli byste si data sami archivovat za delší období."
  },
  {
    "objectID": "posts/2022-04-12-search-console-timeline/index.html#segmentace-podle-url",
    "href": "posts/2022-04-12-search-console-timeline/index.html#segmentace-podle-url",
    "title": "Časové řady z dat Search Console",
    "section": "Segmentace podle URL",
    "text": "Segmentace podle URL\nData Search Console se nejčastěji segmentují podle skupin vstupních stránek (URL) nebo podle skupin dotazů. Většinou segmentuju aktuální stav (tj. např. posledních 28 dní) sloupcovým grafem, ale vývoj v čase se občas hodí taky.\nV tomto konkrétním případě se v URL hned za doménou nachází složka representující kategorii (rubriku či sekci webu), takže se nabízí segmentace stránek podle těchto kategorií. Dále jde segmentovat i podle typu stránky – na webu jsou jednak stránky rubrik se seznamem článků (ty mají za doménou jen jedno lomítko) a jednak články, které mají za doménou dvě lomítka.\n\nTyp stránky: rubriky vs. články\nVyjdu z datasetu sc_by_date_page a doplním ho o sloupec reprezentující typ stránky. Imprese a kliky sečtu za měsíc a rubriku a přidám i počet unikátních stránek. Výsledek vykreslím jako vrstvený plošný graf (stacked area chart). Rubriku poznám tak, že má v URL celkem 3 lomítka, kdežto článek má 4. Tím se mi do rubrik dostane i homepage, ale to mi nevadí.\n\nsc_by_date_page |> \n  filter(\n    between(date, as.Date(\"2021-01-01\"), as.Date(\"2022-03-31\")),\n  ) |> \n  mutate(\n    page_type = if_else(str_count(page, \"/\") == 3, \"rubrika\", \"článek\"),\n    month = floor_date(date, \"month\")\n  ) |> \n  group_by(month, page_type) |> \n  summarise(\n    n = n_distinct(page),\n    impressions = sum(impressions),\n    clicks = sum(clicks),\n    .groups = \"drop\"\n  ) |> \n  pivot_longer(cols = n:last_col(), names_to = \"metric\") |> \n  ggplot(aes(x = month, y = value, fill = page_type)) +\n  geom_area() +\n  facet_wrap(~ metric, ncol = 1, scales = \"free_y\", strip.position = \"right\") +\n  scale_fill_brewer(type = \"qual\", palette = 1)\n\n\n\n\nVýsledek je jednoznačný: rubrik je oproti článkům velmi málo a na viditelnosti nebo návštěvnosti z Googlu se reálně nepodílejí.\n\n\nRubriky\nRubriky identifikuje první složka za doménou, takže ji stačí z URL vykousnout funkcí str_extract z balíčku stringr, které pošlu jen cestu za doménou získanou funkcí path z balíčku urltools. Homepage vyřadím (má ve sloupci category hodnotu NA).\n\nsc_by_date_page |> \n  mutate(\n    category = str_extract(path(page), \"^[^/]+\"),\n    month = floor_date(date, \"month\")\n  ) |> \n  filter(\n    between(month, as.Date(\"2021-01-01\"), as.Date(\"2022-03-31\")),\n    !is.na(category)\n  ) |> \n  group_by(month, category) |> \n  summarise(\n    n = n_distinct(page),\n    impressions = sum(impressions),\n    clicks = sum(clicks),\n    .groups = \"drop\"\n  ) |> \n  pivot_longer(cols = n:last_col(), names_to = \"metric\") |> \n  ggplot(aes(x = month, y = value, fill = category)) +\n  geom_area() +\n  facet_wrap(~ metric, ncol = 1, scales = \"free_y\", strip.position = \"right\") +\n  scale_fill_brewer(type = \"qual\", palette = 8)\n\n\n\n\nTady už je interpretace zajímavější.\n\nStránek v rubrice značky je hodně, ale impresí a kliků mají málo. Lze je tedy považovat za nepříliš úspěšné. Obdobně je na tom katalog.\nOpakem jsou trendy, které mají velmi málo stránek, ale získávají hodně impresí i kliků.\nRubriky nákupy a materiály mají hodně impresí, ale z hlediska viditelnosti nákupy fungují efektivněji, protože impresí dosahují s menším počtem stránek. Zase ale mají horší CTR.\nZajímavá je rubrika m (magazín), která má hodně kliků při menším počtu impresí (takže vysoké CTR) a navíc jí kliky v posledních měsících rychle rostou i při stagnujícím počtu stránek.\n\nAby byla efektivita výkonových metrik (kliky a imprese) vzhledem k počtu unikátních stránek vidět ještě lépe, lze použít poměrové metriky: počet impresí na stránku a počet kliků na stránku.\n\nsc_by_date_page |> \n  mutate(\n    category = str_extract(path(page), \"^[^/]+\"),\n    month = floor_date(date, \"month\")\n  ) |> \n  filter(\n    between(month, as.Date(\"2021-01-01\"), as.Date(\"2022-03-31\")),\n    !is.na(category)\n  ) |> \n  group_by(month, category) |> \n  summarise(\n    impressions_pp = sum(impressions) / n_distinct(page),\n    clicks_pp = sum(clicks) / n_distinct(page),\n    .groups = \"drop\"\n  ) |> \n  pivot_longer(cols = impressions_pp:last_col(), names_to = \"metric\") |> \n  ggplot(aes(x = month, y = value, fill = category)) +\n  geom_area() +\n  facet_wrap(~ metric, ncol = 1, scales = \"free_y\", strip.position = \"right\") +\n  scale_fill_brewer(type = \"qual\", palette = 8)\n\n\n\n\nZde je ještě lépe vidět malá efektivita značek a katalogu a vysoká efektivita trendů.\nZ těchto poznatků jde odvodit docela dost doporučení pro další práci s obsahem, ale to již záleží na konkrétním kontextu projektu.\nPodobně jde segmentovat podle dotazů. Nejčastěji se tak rozlišuje brandové hledání od obecného, ale jde zapojit i klasifikace dotazů z analýzy klíčových slov a segementovat podle ní. To už ale přesahuje téma tohoto článku.\nA to je všechno :-)"
  },
  {
    "objectID": "posts/changepoint-detection/index.html",
    "href": "posts/changepoint-detection/index.html",
    "title": "Detekce změny průměru v časové řadě",
    "section": "",
    "text": "Mám časovou řadu nějaké metriky. Chci zjistit, zda a případně kde se v ní nacházejí významné změny v průměru. Na to existuje mnoho metod a balíčků, viz např. srovnání v dokumentci balíčku mcp. Další jde vygooglit třeba dotazem change point detection in R.\nV tomto zápisku použiju balíček changepoint. Nevím, jestli je pro moje účely nejvhodnější, ale používám ho a osvědčil se mi. Zkoušel jsem i jiné, které dávaly buď skoro stejné, nebo horší výsledky. Nicméně je možné, že jsem je neuměl správně nastavit, protože v dokumentaci mívají hodně matematiky, kterou jsem byl líný studovat.\nDále chci vykreslit graf, který změny v průměru hezky ukáže."
  },
  {
    "objectID": "posts/changepoint-detection/index.html#balíčky",
    "href": "posts/changepoint-detection/index.html#balíčky",
    "title": "Detekce změny průměru v časové řadě",
    "section": "Balíčky",
    "text": "Balíčky\n\nlibrary(tidyverse)\nlibrary(changepoint)"
  },
  {
    "objectID": "posts/changepoint-detection/index.html#vstupní-data",
    "href": "posts/changepoint-detection/index.html#vstupní-data",
    "title": "Detekce změny průměru v časové řadě",
    "section": "Vstupní data",
    "text": "Vstupní data\nJako vstupní data použiju metriky Search Console jednoho webu s dimenzí date. Web je anonymní, ale jedná se o reálný případ, který jsem nedávno řešil pro klienta. Data jsem si předem stáhl a uložil do souboru, který teď jen načtu.\n\nsc_date <- read_rds(\"data-raw/sc_date.rds\")\n\nData mají tuto strukturu:\n\nsc_date |> glimpse()\n\nRows: 56\nColumns: 5\n$ date        <date> 2022-03-18, 2022-03-19, 2022-03-20, 2022-03-21, 2022-03-2…\n$ clicks      <int> 4428, 3285, 3693, 4420, 5505, 4060, 4035, 3374, 2944, 3017…\n$ impressions <int> 11825, 9857, 10523, 11969, 13918, 11034, 10891, 9586, 8807…\n$ ctr         <dbl> 0.3744609, 0.3332657, 0.3509455, 0.3692873, 0.3955310, 0.3…\n$ position    <dbl> 3.264355, 3.439992, 3.623776, 3.236193, 3.121354, 3.589179…\n\n\nZajímají mě imprese a ty se v čase vyvíjely takhle:\n\nsc_date |> \n  ggplot(aes(date, impressions)) +\n  geom_line() +\n  scale_x_date(date_breaks = \"2 weeks\")"
  },
  {
    "objectID": "posts/changepoint-detection/index.html#nalezení-změn-v-průměru",
    "href": "posts/changepoint-detection/index.html#nalezení-změn-v-průměru",
    "title": "Detekce změny průměru v časové řadě",
    "section": "Nalezení změn v průměru",
    "text": "Nalezení změn v průměru\n\nJedna změna\nSamotné nalezení jedné změny v průměru je s balíčkem changepoint a jeho funkcí cpt.mean jednoduché. Stačí numerický vektor a spolehnout se na výchozí hodnoty parametrů.\n\ncpt_object <- cpt.mean(sc_date$impressions)\nsummary(cpt_object)\n\nCreated Using changepoint version 2.2.4 \nChangepoint type      : Change in mean \nMethod of analysis    : AMOC \nTest Statistic  : Normal \nType of penalty       : MBIC with value, 12.07606 \nMinimum Segment Length : 1 \nMaximum no. of cpts   : 1 \nChangepoint Locations : 35 \n\n\nVýsledek jde i vykreslit základní funkcí plot.\n\nplot(cpt_object)\n\n\n\n\n\n\nVíc změn\nPředešlý postup detekuje jen jednu (největší) změnu, ale co když chci detekovat víc změn? Aby funkce cpt.mean našla víc změn, je třeba nastavit několik parametrů. Postupoval jsem podle článku Comparison of change point detection methods.\n\ncpt_object <- cpt.mean(\n  data = sc_date$impressions,\n  penalty = \"AIC\",\n  method = \"PELT\",\n  Q = 5,\n  minseglen = 14\n)\nsummary(cpt_object)\n\nCreated Using changepoint version 2.2.4 \nChangepoint type      : Change in mean \nMethod of analysis    : PELT \nTest Statistic  : Normal \nType of penalty       : AIC with value, 4 \nMinimum Segment Length : 14 \nMaximum no. of cpts   : Inf \nChangepoint Locations : 17 35 \n\n\n\nplot(cpt_object)"
  },
  {
    "objectID": "posts/changepoint-detection/index.html#hezčí-výstupu",
    "href": "posts/changepoint-detection/index.html#hezčí-výstupu",
    "title": "Detekce změny průměru v časové řadě",
    "section": "Hezčí výstupu",
    "text": "Hezčí výstupu\nNevýhoda je, že ve standardním výstupu jsou body změny identifikovány jen pořadovým číslem v časové řadě a graf je dost ošklivý. Chci určit přesná data a graf vykreslit balíčkem ggplot2.\n\nDešifrování třídy cpt\nFunkce cpt.mean vrací objekt třídy cpt. Vypadá takhle:\n\ncpt_object |> str()\n\nFormal class 'cpt' [package \"changepoint\"] with 12 slots\n  ..@ data.set : Time-Series [1:56] from 1 to 56: 11825 9857 10523 11969 13918 11034 10891 9586 8807 8864 ...\n  ..@ cpttype  : chr \"mean\"\n  ..@ method   : chr \"PELT\"\n  ..@ test.stat: chr \"Normal\"\n  ..@ pen.type : chr \"AIC\"\n  ..@ pen.value: num 4\n  ..@ minseglen: num 14\n  ..@ cpts     : int [1:3] 17 35 56\n  ..@ ncpts.max: num Inf\n  ..@ param.est:List of 1\n  .. ..$ mean: num [1:3] 10864 11794 10140\n  ..@ date     : chr \"Thu Feb 16 04:18:49 2023\"\n  ..@ version  : chr \"2.2.4\"\n\n\nZ toho mě zajímá počet change pointů, jejich pozice a průměry odpovídající segmentům mezi change pointy.\n\nlength(cpt_object@cpts)\n\n[1] 3\n\ncpt_object@cpts[1:length(cpt_object@cpts)]\n\n[1] 17 35 56\n\ncpt_object@param.est$mean\n\n[1] 10864.41 11794.50 10139.90\n\n\nData, které odpovídají pozicím, najdu snadno:\n\nsc_date$date[cpt_object@cpts[1:length(cpt_object@cpts)]]\n\n[1] \"2022-04-03\" \"2022-04-21\" \"2022-05-12\"\n\n\n\n\ncpt –> tibble\nZ toho už dokážu sestavit funkci, která vrátí přehledný data frame segmentů.\n\ncpt_as_tibble <- function(cpt, dates) {\n  cpt_dates <- dates[cpt@cpts[1:length(cpt@cpts) - 1]]\n  tibble(\n    start_date = c(min(dates), cpt_dates + 1),\n    end_date = c(cpt_dates, max(dates)),\n    value = cpt@param.est$mean\n  )\n}\n\ncpt_as_tibble(cpt_object, sc_date$date)\n\n\n\n\n\nstart_date\nend_date\nvalue\n\n\n\n\n2022-03-18\n2022-04-03\n10864.41\n\n\n2022-04-04\n2022-04-21\n11794.50\n\n\n2022-04-22\n2022-05-12\n10139.90\n\n\n\n\n\n\n\n\nGraf pomocí ggplot\nA teď již můžu vykreslit hezčí graf.\n\ncpt_plot <- function(cpt, dates) {\n  segment <- cpt_as_tibble(cpt, dates)\n  \n  tibble(date = dates, value = as.numeric(cpt_object@data.set)) |> \n    ggplot(aes(date, value)) +\n    geom_line() +\n    geom_segment(\n      data = segment,\n      aes(x = start_date, xend = end_date, y = value, yend = value), \n      color = \"red\"\n    ) +\n    scale_x_date(breaks = c(segment$start_date, max(dates)))\n}\n\ncpt_plot(cpt_object, sc_date$date)\n\n\n\n\nJeště ověřím, že funkce správně pracují i s jednou změnou.\n\ncpt_plot(cpt.mean(sc_date$impressions), sc_date$date)\n\n\n\n\nA to je všechno :-)"
  },
  {
    "objectID": "posts/detekce-azbuky/index.html",
    "href": "posts/detekce-azbuky/index.html",
    "title": "Jak se zbavit azbuky, čínštiny apod.",
    "section": "",
    "text": "Mám tahle data:\n\nlibrary(tidyverse)\n\ndf <- tibble(\n  text = c(\n    \"Praha - Střešovice\",\n    \"Prague - Stresovice\",\n    \"布拉格 - Střešovice\",\n    \"Прага - Střešovice\",\n    \"Πράγα - Střešovice\"\n  ), \n  jazyk = c(\"česky\", \"anglicky\", \"čínsky\", \"rusky\", \"řecky\")\n)\n\ndf\n\n\n\n\n\ntext\njazyk\n\n\n\n\nPraha - Střešovice\nčesky\n\n\nPrague - Stresovice\nanglicky\n\n\n布拉格 - Střešovice\nčínsky\n\n\nПрага - Střešovice\nrusky\n\n\nΠράγα - Střešovice\nřecky\n\n\n\n\n\n\n\n\n\ndf |> \n  filter(!str_detect(text, \"[\\\\p{Cyrillic}]\"))\n\n\n\n\n\ntext\njazyk\n\n\n\n\nPraha - Střešovice\nčesky\n\n\nPrague - Stresovice\nanglicky\n\n\n布拉格 - Střešovice\nčínsky\n\n\nΠράγα - Střešovice\nřecky\n\n\n\n\n\n\n\n\n\n\ndf |> \n  filter(!str_detect(text, \"[\\\\p{Han}]\"))\n\n\n\n\n\ntext\njazyk\n\n\n\n\nPraha - Střešovice\nčesky\n\n\nPrague - Stresovice\nanglicky\n\n\nПрага - Střešovice\nrusky\n\n\nΠράγα - Střešovice\nřecky\n\n\n\n\n\n\n\n\n\n\ndf |> \n  filter(!str_detect(text, \"[\\\\p{Greek}]\"))\n\n\n\n\n\ntext\njazyk\n\n\n\n\nPraha - Střešovice\nčesky\n\n\nPrague - Stresovice\nanglicky\n\n\n布拉格 - Střešovice\nčínsky\n\n\nПрага - Střešovice\nrusky\n\n\n\n\n\n\n\n\n\nTo už je těžší. V předešlých případech jsem odstranil texty, které obsahovaly alespoň jeden znak z daného Unicode rozsahu, ale zde musím odstranit texty, které obsahují jenom znaky z daného rozsahu. Musím tedy zkombinovat tři část:\n\n{Latin} – latinkové znaky\n{Punct} – interpunkce\n\\x20 – mezera (může být napsaná i normálně, ale byla by blbě vidět)\n\n\ndf |> \n  filter(str_detect(text, \"^[\\x20\\\\p{Punct}\\\\p{Latin}]+$\"))\n\n\n\n\n\ntext\njazyk\n\n\n\n\nPraha - Střešovice\nčesky\n\n\nPrague - Stresovice\nanglicky\n\n\n\n\n\n\n\n\n\nSamozřejmě to jde použít i pro hledání textů s určitými znaky.\nVšechny v azbuce:\n\ndf |> \n  filter(str_detect(text, \"[\\\\p{Cyrillic}]\"))\n\n\n\n\n\ntext\njazyk\n\n\n\n\nПрага - Střešovice\nrusky\n\n\n\n\n\n\nVšechny nelatinkové:\n\ndf |> \n  filter(!str_detect(text, \"^[\\x20\\\\p{Punct}\\\\p{Latin}]+$\"))\n\n\n\n\n\ntext\njazyk\n\n\n\n\n布拉格 - Střešovice\nčínsky\n\n\nПрага - Střešovice\nrusky\n\n\nΠράγα - Střešovice\nřecky\n\n\n\n\n\n\nV případě potřeby něčeho speciálního poslouží podrobný přehled Unicode v regulárních výrazech."
  },
  {
    "objectID": "posts/r-pro-digitalni-marketing/index.html",
    "href": "posts/r-pro-digitalni-marketing/index.html",
    "title": "Děláte digitální marketing a chcete začít používat R? Tohle je váš studijní plán",
    "section": "",
    "text": "Základem tohoto článku je sylabus, ktarý jsem si vyrobil pro svůj workshop Jak dělat SEO lépe a rychleji v jazyce R. Jenže jsem měl jsem velké oči. Za jediný den, který jsem měl k dispozici, ho nešlo probrat ani náhodou. Takže jsem ho trochu rozpracoval, doplnil odkazy na zdroje a předkládám vám ho jako orientační mapu pro samostudium.\nSnažil jsem se témata řadit podle priority a zároveň obtížnosti. Myslím, že vám pomůže, když pořadí dodržíte, ale postupujte samozřejmě podle svých výchozích znalostí a svého uvážení.\nDo studia vám držím palce!"
  },
  {
    "objectID": "posts/r-pro-digitalni-marketing/index.html#základy-r",
    "href": "posts/r-pro-digitalni-marketing/index.html#základy-r",
    "title": "Děláte digitální marketing a chcete začít používat R? Tohle je váš studijní plán",
    "section": "Základy R",
    "text": "Základy R\nNaučte se základní datové typy: čísla, znakové řetězce, logické hodnoty. Zvykněte si na to, že to jsou vektory a naučte se s nimi pracovat. Nechoďte zbytečně do hloubky. Pro začátek vám stačí, když pochopíte a v případě potřeby i napíšete třeba tyto příklady kódu:\nvysledek_vypoctu <- (5 * 4) / 2\nspojene_retezce <- paste(\"první řetězec\", \"druhý řetězec\", sep = \",\")\nvynasobeny_vektor <- c(1, 2, 3, 6, 5, 4) * 10\ndruhy_prvek_vektoru <- vynasobeny_vektor[2]\nvynasobene_vektor <- c(1, 2, 3) * c(1, 2, 3)\nzaokrouhlena_nahodna_cisla <- runif(n = 40, min = 1, max = 10) |> \n  round(digits = 1)\nK základním datovým typům přidejte ještě data.frame a případně list. Zase stačí jen málo:\ndf <- data.frame(\n  cislo = 1:3,\n  text = c(\"první\", \"druhý\", \"třetí\")\n)\nprvni_text <- df[1, 2]\nvsechny_texty <- df$text\nNezpomeňte, že kdykoli můžete R požádat o nápovědu – např. ?sum nebo klávesou F1 s kurzorem na funkci."
  },
  {
    "objectID": "posts/r-pro-digitalni-marketing/index.html#základy-rstudia-a-quarta",
    "href": "posts/r-pro-digitalni-marketing/index.html#základy-rstudia-a-quarta",
    "title": "Děláte digitální marketing a chcete začít používat R? Tohle je váš studijní plán",
    "section": "Základy RStudia a Quarta",
    "text": "Základy RStudia a Quarta\nPrakticky všechno budete dělat v RStudiu. Nainstalujte si ho, udržujte ho aktuální, naučte se s ním dobře zacházet.\nErkový kód pište prakticky výhradně do Quarto dokumentů. Skripty zatím nebudete potřebovat, starší R Markdown nebo R Notebook ignorujte. Studujte postupně z:\n\nTutorial: Hello, Quarto\nTutorial: Computations\nTutorial: Authoring\n\nPro začátek to nepřehánějte, jde jen o to, abyste měli kde interaktivně pracovat a viděli současně kód i jeho výsledek. Reporty a další výstupy z Quarta se naučíte později."
  },
  {
    "objectID": "posts/r-pro-digitalni-marketing/index.html#tidyverse",
    "href": "posts/r-pro-digitalni-marketing/index.html#tidyverse",
    "title": "Děláte digitální marketing a chcete začít používat R? Tohle je váš studijní plán",
    "section": "Tidyverse",
    "text": "Tidyverse\nPro práci s daty používejte zásadně balíčky z ekosystému Tidyverse. Konkrétně se naučte a na praktických příkladech si vyzkoušejte následující funkce.\n\nVytváření data framů\n\ntibble a trible na vytvoření data framu\nglimpse na prohlédnutí struktury data framu\n\n\n\nImport a export dat\n\nread_csv etc. na import textových souborů\nwrite_csv etc. na export do textových soubor\nread_excel etc. na import z Excelu\nwrite_rds a read_rds, pokud si chcete sami něco uložit v erkovém interním formátu\npokud potřebujete, balíček googlesheets4 na čtení z a zápis do Tabulek Google\n\n\n\nManipulace s daty\n\nNaučte se a hlavně si vyzkoušejte všechno, co najdete v taháku k balíčku dplyr.\nNakoukněte i do taháku k balíčku tidyr. Ten se neučte, jen si přibližně zapamatujte, co umí. Až to budete potřebovat, vygooglíte si to.\n\n\n\nGrafy balíčkem ggplot2\nNeztrácejte čas funkcí plot ze základního R. Na vizualizace používejte jen balíček gglot2. Naučte se a hlavně si vyzkoušejte všechno, co najdete v jeho taháku."
  },
  {
    "objectID": "posts/r-pro-digitalni-marketing/index.html#příručka-od-excelu-k-r",
    "href": "posts/r-pro-digitalni-marketing/index.html#příručka-od-excelu-k-r",
    "title": "Děláte digitální marketing a chcete začít používat R? Tohle je váš studijní plán",
    "section": "Příručka od Excelu k R",
    "text": "Příručka od Excelu k R\nVýše uvedená témata tak akorát pokrývá moje příručka Od Excelu k R. Projděte si ji a rovnou z ní všechno zkoušejte, ušetříte tím dost času. Jen pozor: teď už použijte Quarto Document namísto R Notebook."
  },
  {
    "objectID": "posts/r-pro-digitalni-marketing/index.html#digitální-marketing",
    "href": "posts/r-pro-digitalni-marketing/index.html#digitální-marketing",
    "title": "Děláte digitální marketing a chcete začít používat R? Tohle je váš studijní plán",
    "section": "Digitální marketing",
    "text": "Digitální marketing\nZe specialit digitálního marketingu doporučuju začít těmito tématy:\n\nAnalýza dat Search Console – použijete balíček searchConsoleR v kombinaci s balíčky tidyr, dplyr a ggplot2.\nNa Google Analytics použijete balíček googleAnalyticsR.\nDále doporučuju (polo)automatizovat reportování ze Screaming Frogu. Stačí načíst jeho CSV exporty (balíček readr), zpracovat (balíček dplyr), vizualizovat (balíček ggplot2) a reportovat v Quarto.\nPokud děláte SEO a analýzy klíčových slov, naučte se můj balíček keywordr."
  },
  {
    "objectID": "posts/r-pro-digitalni-marketing/index.html#reporting",
    "href": "posts/r-pro-digitalni-marketing/index.html#reporting",
    "title": "Děláte digitální marketing a chcete začít používat R? Tohle je váš studijní plán",
    "section": "Reporting",
    "text": "Reporting\nJestli máte klienty a/nebo šéfy, naučte se hodně dobře Quarto a generování reportů, případně i prezentací. Ušetří vám to mraky času."
  },
  {
    "objectID": "posts/r-pro-digitalni-marketing/index.html#stahování-informací-z-webu",
    "href": "posts/r-pro-digitalni-marketing/index.html#stahování-informací-z-webu",
    "title": "Děláte digitální marketing a chcete začít používat R? Tohle je váš studijní plán",
    "section": "Stahování informací z webu",
    "text": "Stahování informací z webu\n\nObčas se hodí stáhnout něco z webu. Na to je perfektní balíček rvest.\nJestli chcete cucat data z různých API, zkuste balíček httr. Tím jde taky ověřovat stavové kódy HTTP.\nNa import XML sitemap se kromě balíčku httr hodí i balíček xml2.\n\nA to je vlastně všechno. Leda byste byli stejně velcí blázni jako já a pustili se i do Shiny :-)"
  },
  {
    "objectID": "posts/analyza-url/index.html",
    "href": "posts/analyza-url/index.html",
    "title": "Analýza struktury většího počtu URL",
    "section": "",
    "text": "V dataframu, typicky třeba ze Screaming Frogu nebo Search Console, mám proměnnou obsahující URL. Chci přehledně zobrazit, z jakých částí se tato URL skládají."
  },
  {
    "objectID": "posts/analyza-url/index.html#co-na-to-potřebuju",
    "href": "posts/analyza-url/index.html#co-na-to-potřebuju",
    "title": "Analýza struktury většího počtu URL",
    "section": "Co na to potřebuju",
    "text": "Co na to potřebuju\nKromě obligátní tidyverse mi pomůže balíček urltools.\n\nlibrary(tidyverse)\nlibrary(urltools)"
  },
  {
    "objectID": "posts/analyza-url/index.html#příklad-vstupních-dat",
    "href": "posts/analyza-url/index.html#příklad-vstupních-dat",
    "title": "Analýza struktury většího počtu URL",
    "section": "Příklad vstupních dat",
    "text": "Příklad vstupních dat\nPomocí ChatGPT jsem vygeneroval vzorek náhodných URL a uložil do souboru url.csv.\n\nsample_df <- read_csv(\"url.csv\")\n\nRows: 81 Columns: 1\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): url\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsample_df\n\n# A tibble: 81 × 1\n   url                                                           \n   <chr>                                                         \n 1 http://example.com/index.html                                 \n 2 https://blog.example.com/archive#february                     \n 3 http://example.net/about.html                                 \n 4 https://secure.example.org/search?q=keyword                   \n 5 http://example.com/products/item1.html                        \n 6 https://secure.example.com/login?return_url=/account/dashboard\n 7 http://example.net/news/2023/02/17/headline                   \n 8 https://shop.example.com/cart?item=12345                      \n 9 http://example.org/resources/guide.pdf                        \n10 https://login.example.com/                                    \n# … with 71 more rows"
  },
  {
    "objectID": "posts/analyza-url/index.html#rozložení-url",
    "href": "posts/analyza-url/index.html#rozložení-url",
    "title": "Analýza struktury většího počtu URL",
    "section": "Rozložení URL",
    "text": "Rozložení URL\nURL nejprve rozložím na jednotlivé části podle RFC 3986. Zároveň přejmenuju některé sloupce tak, aby odpovídaly RFC.\n\nsample_df <- read_csv(\"url.csv\") |> \n  bind_cols(url_parse(sample_df$url)) |> \n  rename(host = domain) |> \n  rename(query = parameter)\nsample_df\n\n# A tibble: 81 × 7\n   url                                    scheme host  port  path  query fragm…¹\n   <chr>                                  <chr>  <chr> <chr> <chr> <chr> <chr>  \n 1 http://example.com/index.html          http   exam… <NA>  inde… <NA>  <NA>   \n 2 https://blog.example.com/archive#febr… https  blog… <NA>  arch… <NA>  februa…\n 3 http://example.net/about.html          http   exam… <NA>  abou… <NA>  <NA>   \n 4 https://secure.example.org/search?q=k… https  secu… <NA>  sear… q=ke… <NA>   \n 5 http://example.com/products/item1.html http   exam… <NA>  prod… <NA>  <NA>   \n 6 https://secure.example.com/login?retu… https  secu… <NA>  login retu… <NA>   \n 7 http://example.net/news/2023/02/17/he… http   exam… <NA>  news… <NA>  <NA>   \n 8 https://shop.example.com/cart?item=12… https  shop… <NA>  cart  item… <NA>   \n 9 http://example.org/resources/guide.pdf http   exam… <NA>  reso… <NA>  <NA>   \n10 https://login.example.com/             https  logi… <NA>  <NA>  <NA>  <NA>   \n# … with 71 more rows, and abbreviated variable name ¹​fragment"
  },
  {
    "objectID": "posts/analyza-url/index.html#statistika-protokolů-a-hostnames",
    "href": "posts/analyza-url/index.html#statistika-protokolů-a-hostnames",
    "title": "Analýza struktury většího počtu URL",
    "section": "Statistika protokolů a hostnames",
    "text": "Statistika protokolů a hostnames\nZ rozložených dat už snadno spočítám jednotlivé složky. Pro vizualizaci použiju ggplot s geomem geom_bar, který automaticky ukazuje počty, takže není třeba používat funkce count nebo n. Aby byly grafy seřazené od nejvyššího počtu po nejnižší, použiju fct_infreq a případně fct_rev z balíčku forcats.\n\nProtokoly\n\nsample_df |> \n  ggplot(aes(x = scheme)) +\n  geom_bar()\n\n\n\n\n\n\nHostnames\n\nsample_df |> \n  ggplot(aes(y = fct_rev(fct_infreq(host)))) +\n  geom_bar() +\n  labs(y = NULL)\n\n\n\n\n\n\nJen domény druhého řádu\nDoménu druhého řádu vykousnu z hostname regulárním výrazem.\n\nsample_df |> \n  transmute(domain = str_extract(host, \"[^.]+\\\\.[^.]+$\")) |> \n  ggplot(aes(x = fct_infreq(domain))) +\n  geom_bar() +\n  labs(x = NULL)"
  },
  {
    "objectID": "posts/analyza-url/index.html#rozložení-cesty-na-složky",
    "href": "posts/analyza-url/index.html#rozložení-cesty-na-složky",
    "title": "Analýza struktury většího počtu URL",
    "section": "Rozložení cesty na složky",
    "text": "Rozložení cesty na složky\nNejprve musím zjistit maximální počet lomítek.\n\nmax_level <- sample_df |> \n  mutate(level = str_count(path, \"/\") + 1) |> \n  pull(level) |> \n  max(na.rm = TRUE)\n\nNyní rozložím cestu pomocí tidyr::separate.\n\nsample_df |> \n  select(path) |> \n  separate(path, into = paste0(\"l\", 1:max_level), sep = \"/\", fill = \"right\")\n\n# A tibble: 81 × 5\n   l1         l2         l3    l4    l5      \n   <chr>      <chr>      <chr> <chr> <chr>   \n 1 index.html <NA>       <NA>  <NA>  <NA>    \n 2 archive    <NA>       <NA>  <NA>  <NA>    \n 3 about.html <NA>       <NA>  <NA>  <NA>    \n 4 search     <NA>       <NA>  <NA>  <NA>    \n 5 products   item1.html <NA>  <NA>  <NA>    \n 6 login      <NA>       <NA>  <NA>  <NA>    \n 7 news       2023       02    17    headline\n 8 cart       <NA>       <NA>  <NA>  <NA>    \n 9 resources  guide.pdf  <NA>  <NA>  <NA>    \n10 <NA>       <NA>       <NA>  <NA>  <NA>    \n# … with 71 more rows\n\n\nPro rozloženou cestu už jde spočítat jakákoli statistika, např. nejčastější složky první úrovně.\n\nsample_df |> \n  select(path) |> \n  separate(path, into = paste0(\"l\", 1:max_level), sep = \"/\", fill = \"right\") |> \n  count(l1) |> \n  drop_na() |> \n  slice_max(n, n = 10)\n\n# A tibble: 10 × 2\n   l1             n\n   <chr>      <int>\n 1 products      10\n 2 contact-us     6\n 3 about          5\n 4 account        5\n 5 login          5\n 6 news           5\n 7 services       5\n 8 blog           4\n 9 resources      4\n10 search         4"
  },
  {
    "objectID": "posts/analyza-url/index.html#rozložení-parametrů",
    "href": "posts/analyza-url/index.html#rozložení-parametrů",
    "title": "Analýza struktury většího počtu URL",
    "section": "Rozložení parametrů",
    "text": "Rozložení parametrů\nJednotlivé parametry jde získat funkcí urltools::param_get.\n\nsample_df |> \n  pull(url) |> \n  param_get() |> \n  slice_head(n = 10)\n\n   color  item       q redirect         return_url size status\n1   <NA>  <NA>    <NA>     <NA>               <NA> <NA>   <NA>\n2   <NA>  <NA>    <NA>     <NA>               <NA> <NA>   <NA>\n3   <NA>  <NA>    <NA>     <NA>               <NA> <NA>   <NA>\n4   <NA>  <NA> keyword     <NA>               <NA> <NA>   <NA>\n5   <NA>  <NA>    <NA>     <NA>               <NA> <NA>   <NA>\n6   <NA>  <NA>    <NA>     <NA> /account/dashboard <NA>   <NA>\n7   <NA>  <NA>    <NA>     <NA>               <NA> <NA>   <NA>\n8   <NA> 12345    <NA>     <NA>               <NA> <NA>   <NA>\n9   <NA>  <NA>    <NA>     <NA>               <NA> <NA>   <NA>\n10  <NA>  <NA>    <NA>     <NA>               <NA> <NA>   <NA>\n\n\nZískanou širokou tabulku lze pak převrátit na dlouhou a spočítat statistiku parametrů i jejich hodnot.\n\nsample_df |> \n  pull(url) |> \n  param_get() |> \n  pivot_longer(cols = everything()) |> \n  drop_na() |>\n  count(name, sort = TRUE)\n\n# A tibble: 7 × 2\n  name           n\n  <chr>      <int>\n1 q              4\n2 status         3\n3 color          2\n4 item           2\n5 redirect       2\n6 return_url     2\n7 size           2\n\n\n\nsample_df |> \n  pull(url) |> \n  param_get() |> \n  pivot_longer(cols = everything()) |> \n  drop_na() |> \n  group_by(name) |> \n  summarise(\n    n = n(),\n    values = paste(unique(value), collapse = \", \")\n  ) |> \n  arrange(desc(n))\n\n# A tibble: 7 × 3\n  name           n values                        \n  <chr>      <int> <chr>                         \n1 q              4 keyword                       \n2 status         3 shipped, pending, completed   \n3 color          2 red, blue                     \n4 item           2 12345, 67890                  \n5 redirect       2 /dashboard                    \n6 return_url     2 /account/dashboard, /dashboard\n7 size           2 medium, large                 \n\n\nA to je všechno :-)"
  }
]