[
  {
    "path": "posts/detekce-azbuky/",
    "title": "Jak se zbavit azbuky, čínštiny apod.",
    "description": "Do dat se občas dostane nechtěná ruština, čínština, řečtina, nebo jiný jazyk psaný nelatinkovým písmem. Tohle je rychlý návod, jak se takových textů pomocí regulárních výrazů zbavit, nebo je naopak najít.",
    "author": [
      {
        "name": "Marek Prokop",
        "url": {}
      }
    ],
    "date": "2022-05-01",
    "categories": [],
    "contents": "\r\nRychlý příklad\r\nMám tahle data:\r\n\r\n\r\ndf <- tibble(\r\n  text = c(\r\n    \"Praha - Střešovice\",\r\n    \"Prague - Stresovice\",\r\n    \"布拉格 - Střešovice\",\r\n    \"Прага - Střešovice\",\r\n    \"Πράγα - Střešovice\"\r\n  ), \r\n  jazyk = c(\"česky\", \"anglicky\", \"čínsky\", \"rusky\", \"řecky\")\r\n)\r\n\r\ndf\r\n\r\n\r\ntext\r\njazyk\r\nPraha - Střešovice\r\nčesky\r\nPrague - Stresovice\r\nanglicky\r\n布拉格 - Střešovice\r\nčínsky\r\nПрага - Střešovice\r\nrusky\r\nΠράγα - Střešovice\r\nřecky\r\n\r\nChci odstranit texty v azbuce\r\n\r\n\r\ndf |> \r\n  filter(!str_detect(text, \"[\\\\p{Cyrillic}]\"))\r\n\r\n\r\ntext\r\njazyk\r\nPraha - Střešovice\r\nčesky\r\nPrague - Stresovice\r\nanglicky\r\n布拉格 - Střešovice\r\nčínsky\r\nΠράγα - Střešovice\r\nřecky\r\n\r\nChci odstranit texty v čínštině\r\n\r\n\r\ndf |> \r\n  filter(!str_detect(text, \"[\\\\p{Han}]\"))\r\n\r\n\r\ntext\r\njazyk\r\nPraha - Střešovice\r\nčesky\r\nPrague - Stresovice\r\nanglicky\r\nПрага - Střešovice\r\nrusky\r\nΠράγα - Střešovice\r\nřecky\r\n\r\nChci odstranit texty v řečtině\r\n\r\n\r\ndf |> \r\n  filter(!str_detect(text, \"[\\\\p{Greek}]\"))\r\n\r\n\r\ntext\r\njazyk\r\nPraha - Střešovice\r\nčesky\r\nPrague - Stresovice\r\nanglicky\r\n布拉格 - Střešovice\r\nčínsky\r\nПрага - Střešovice\r\nrusky\r\n\r\nChci odstranit texty v čekmoli kromě latinky\r\nTo už je těžší. V předešlých případech jsem odstranil texty, které obsahovaly alespoň jeden znak z daného Unicode rozsahu, ale zde musím odstranit texty, které obsahují jenom znaky z daného rozsahu. Musím tedy zkombinovat tři část:\r\n{Latin} – latinkové znaky\r\n{Punct} – interpunkce\r\n\\x20 – mezera (může být napsaná i normálně, ale byla by blbě vidět)\r\n\r\n\r\ndf |> \r\n  filter(str_detect(text, \"^[\\x20\\\\p{Punct}\\\\p{Latin}]+$\"))\r\n\r\n\r\ntext\r\njazyk\r\nPraha - Střešovice\r\nčesky\r\nPrague - Stresovice\r\nanglicky\r\n\r\nChci najít…\r\nSamozřejmě to jde použít i pro hledání textů s určitými znaky.\r\nVšechny v azbuce:\r\n\r\n\r\ndf |> \r\n  filter(str_detect(text, \"[\\\\p{Cyrillic}]\"))\r\n\r\n\r\ntext\r\njazyk\r\nПрага - Střešovice\r\nrusky\r\n\r\nVšechny nelatinkové:\r\n\r\n\r\ndf |> \r\n  filter(!str_detect(text, \"^[\\x20\\\\p{Punct}\\\\p{Latin}]+$\"))\r\n\r\n\r\ntext\r\njazyk\r\n布拉格 - Střešovice\r\nčínsky\r\nПрага - Střešovice\r\nrusky\r\nΠράγα - Střešovice\r\nřecky\r\n\r\nV případě potřeby něčeho speciálního poslouží podrobný přehled Unicode v regulárních výrazech.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-05-01T11:19:40+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-04-12-search-console-timeline/",
    "title": "Časové řady z dat Search Console",
    "description": "Jako první na vás v Search Consoli vyskočí graf výkonu webu v čase. Časové řady ze Search Console jdou ale vizualizovat o dost lépe a jdou z nich pak vypozorovat zajímavé věci.",
    "author": [
      {
        "name": "Marek Prokop",
        "url": {}
      }
    ],
    "date": "2022-04-18",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nPříprava\r\nPotřebné balíčky\r\nZákladní parametry\r\nNačtení vstupních dat\r\nNekonzistence dat Search Console\r\n\r\nVizualizace časových řad\r\nZákladní metriky\r\nPočet stránek a dotazů\r\nZoomování grafu\r\n\r\nGranularita období\r\nAgregace za kalendářní období\r\nPrůměr za kalendářní týdny\r\nRozsah hodnot za týden\r\nKlouzavý průměr (moving average)\r\nRegrese\r\n\r\nPoměrové metriky\r\nMetriky podle pozic\r\nPočet dotazů podle intervalů pozic\r\nPočet vstupních stránek podle intervalů pozic\r\nImprese podle intervalů pozic\r\n\r\nPorovnání metrik s předešlým obdobím nebo rokem\r\nMeziroční porovnání (YoY)\r\nPosledních 12 týdnů a 12 týdnů před tím\r\nAutokorelace\r\n\r\nDny v týdnu\r\nDekompozice časové řady\r\n\r\nSegmentace podle URL\r\nTyp stránky: rubriky vs. články\r\nRubriky\r\n\r\n\r\nČlánek je hodně dlouhý, protože sestavit a vizualizovat časové řady (time series) z dat Search Console jde mnoha způsoby a není úplně triviální si vybrat ty, které dávají pro daný účel smysl. Ukážu zde všechny, které bežně používám, i ty, které používám jen výjimečně nebo skoro vůbec.\r\nKód v jazyce R jsem se snažil napsat tak, aby byl funkční, čistý, srozumitelný a hlavně snadno a univerzálně znovupoužitelný. Časem asi na analýzu dat Search Console udělám samostatný balíček, ale do té doby musí stačit tohle.\r\nNa druhou stranu má článek i čistě metodickou, na programovacím jazyce nezávislou rovinu. Některé uvedené časové řady a jejich vizualizace jdou vytvořit i jinak, např. v Data Studiu (na příklad odkazuju dál) nebo v Google Sheets. I bez znalosti R jde tedy článek využít jako vzorník výstupů, které jdou z dat Search Console různými prostředky udělat, a které dávají určitý smysl.\r\nVšechny ukázky čerpají z dat webu Glamour Cabaret Alžběty Faltysové. Jako hobby projekt bez větších investic sice neohromí gigantickými čísly, ale zároveň hezky ukazuje některé vzorce typické pro přirozeně a dobře budovaný obsah se zajímavým potenciálem dalšího růstu. Alžbětě moc děkuju, že mi data pro článek poskytla.\r\nPro jistotu upozorňuju, že tenhle článek se primárně zabývá konstrukcí a vizualizací časových řad z dat Search Console, nikoli jejich detailní analýzou. Té se budu věnnovat někdy jindy.\r\nPříprava\r\nPotřebné balíčky\r\nJako vždy tidyverse, searchConsoleR pro práci se Search Consolí, lubridate pro práci s daty a časem. Balíček feasts je zde jen pro dekompozici časové řady na trend, sezónnost a šum, a balíček slider umožňuje výpočet klouzavých průměrů. Balíček urltools slouží k rozložení URL na složky a používám ho k segentaci dat podle stránek.\r\nDlužno dodat, že balíčky specializované na časové řady zde používám jen v nejnutnější míře a zatím mi to tak vyhovuje. Na druhou stranu přiznávám, že některé věci by šly řešit elegantněji plným nasazením specializovaných balíčků typu timetk, tsbox, tsibble apod.\r\nPoslední řádek nastavuje začátek týdne na pondělí pro balíček lubridate.\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(searchConsoleR)\r\nlibrary(lubridate)\r\nlibrary(feasts)\r\nlibrary(slider)\r\nlibrary(urltools)\r\n\r\noptions(lubridate.week.start = 1)\r\n\r\n\r\n\r\nZákladní parametry\r\nKdykoli pracuju s daty Search Console, nejprve definuju parametry, které určují, jaká vstupní data chci získat. Konkrétně:\r\nsc_site – z kterého webu,\r\nsc_country – ze které země má hledání pocházet (používám vždy, pokud web primárně cílí na jednu zemi),\r\ndate_from, date_to – za jaké období data chci.\r\n\r\n\r\nsc_site <- \"https://www.glamourcabaret.cz/\"\r\nsc_country <- \"cze\"\r\ndate_from <- as.Date(\"2020-11-01\")\r\ndate_to <- as.Date(\"2022-04-10\")\r\n\r\n\r\n\r\nNastavení časových parametrů balíčkem lubridate\r\nNěkdy jsem líný hledat v kalendáři vhodná data, nebo chci, aby se mi parametry date_from a date_to automaticky aktualizovaly. Na to se hodí balíček lubridate.\r\nPro Search Consoli typicky chci datum před třemi dny, protože poslední tři dny ještě data v Search Consoli nejsou finální.\r\n\r\n\r\ntoday() - 3\r\n\r\n\r\n[1] \"2022-04-16\"\r\n\r\nKdyž chci třeba poslední 4 celé kalendářní týdny, udělám to takhle:\r\n\r\n\r\nperiod_end <- floor_date(today() - 2, unit = \"week\") - 1\r\nperiod_start <- period_end - weeks(4) + 1\r\n\r\ncat(\"Od\", format(period_start, \"%x (%A)\"), \"do\", format(period_end, \"%x (%A)\"))\r\n\r\n\r\nOd 14.3.2022 (pondělí) do 10.4.2022 (neděle)\r\n\r\nObdobně můžu určit třeba posledních 6 celých kalendářních měsíců:\r\n\r\n\r\nperiod_end <- floor_date(today() - 2, unit = \"month\") - 1\r\nperiod_start <- add_with_rollback(period_end, months(-6), roll_to_first = TRUE)\r\n\r\ncat(\"Od\", format(period_start, \"%x\"), \"do\", format(period_end, \"%x\"))\r\n\r\n\r\nOd 1.10.2021 do 31.3.2022\r\n\r\nNebo poslední celý rok (ten ale nebude po dubnu běžného roku fungovat, protože Search Console archivuje jen posledních 16 měsíců):\r\n\r\n\r\nperiod_end <- floor_date(today() - 2, unit = \"year\") - 1\r\nperiod_start <- floor_date(period_end, \"year\")\r\n\r\ncat(\"Od\", format(period_start, \"%x\"), \"do\", format(period_end, \"%x\"))\r\n\r\n\r\nOd 1.1.2021 do 31.12.2021\r\n\r\nNačtení vstupních dat\r\nPak si podle potřeby definuju funkci, která při prvním zavolání s konkrétními parametry načte data funkcí search_analytics z balíčku searchConsoleR a uloží je do souboru. Při druhém a dalším zavolání se stejnými parametry již načítá data z uloženého souboru. To má dvě výhody:\r\nOpakované načtení dat je mnohem rychlejší.\r\nSkript pokaždé pracuje se stejnými daty, i když už ze Search Console zmizela (jsou tam data jen za posledních 16 měsíců).\r\nA dvě nevýhody:\r\nUložená data zabírají místo na disku.\r\nPokud potřebuju čerstvá data nebo s jinými parametry, musím soubory z disku odstranit.\r\nPro účely tohoto zápisku si funkci definuju velmi jednoduše. Jako jediný parametr má dimenze, vše ostatní se bere přímo z globálních objektů definovaných výše. Data ukládá do podsložky raw-data, která již v aktuální složce musí existovat. Pro jiné účely může být funkce složitější.\r\n\r\n\r\nread_sc <- function(dimensions) {\r\n  rds_path <- file.path(\r\n    \".\", \"data-raw\", paste0(\"sc-\", paste(dimensions, collapse = \"-\"), \".rds\")\r\n  )\r\n  row_limit <- as.integer(date_to - date_from) * 500 * (length(dimensions))\r\n\r\n  if (!file.exists(rds_path)) {\r\n    search_analytics(\r\n      siteURL = sc_site,\r\n      startDate = date_from,\r\n      endDate = date_to,\r\n      dimensions = dimensions,\r\n      dimensionFilterExp = paste0(\"country==\", sc_country),\r\n      rowLimit = row_limit\r\n    ) |>\r\n      write_rds(rds_path, compress = \"gz\")\r\n  }\r\n  read_rds(rds_path)\r\n}\r\n\r\n\r\n\r\nNakonec si opravdu stáhnu resp. ze souborů načtu všechna data, která budu ze Search Console potřebovat. V praxi se s tím musí opatrně, protože tahání více dimenzí za delší období z API může trvat docela dlouho. Když nevím, jak budou data konkrétního webu veliká, zkusím nejprve stáhnout jeden typický den nebo týden a podle toho se rozhodnu.\r\nPokud budete kód sami pouštět, funkci src_auth můžete nechat bez parametrů a ona se vás na e-mail zeptá. Já to mám takhle proto, aby to běželo automaticky.\r\n\r\n\r\nscr_auth(email = Sys.getenv(\"MY_GOOGLE_ACCOUNT\"))\r\n\r\nsc_by_date <- read_sc(\"date\")\r\nsc_by_date_page <- read_sc(c(\"date\", \"page\"))\r\nsc_by_date_query <- read_sc(c(\"date\", \"query\"))\r\nsc_by_date_page_query <- read_sc(c(\"date\", \"page\", \"query\"))\r\n\r\n\r\n\r\nVýsledkem je několik dataframů, jejichž strukturu a obsah si mohu rychle zkontrolovat.\r\n\r\n\r\nglimpse(sc_by_date)\r\n\r\n\r\nRows: 498\r\nColumns: 5\r\n$ date        <date> 2020-11-28, 2020-11-29, 2020-11-30, 2020-12-01,~\r\n$ clicks      <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ~\r\n$ impressions <int> 0, 0, 0, 4, 5, 1, 0, 0, 1, 0, 2, 10, 4, 6, 3, 6,~\r\n$ ctr         <dbl> 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0~\r\n$ position    <dbl> 0.000000, 0.000000, 0.000000, 16.500000, 8.40000~\r\n\r\n\r\n\r\nglimpse(sc_by_date_page)\r\n\r\n\r\nRows: 18,626\r\nColumns: 6\r\n$ date        <date> 2021-12-19, 2021-07-02, 2021-04-19, 2021-07-05,~\r\n$ page        <chr> \"https://www.glamourcabaret.cz/znacky/hermes\", \"~\r\n$ clicks      <int> 30, 13, 12, 12, 11, 11, 10, 8, 8, 8, 8, 8, 7, 7,~\r\n$ impressions <int> 657, 272, 406, 100, 374, 768, 295, 91, 226, 105,~\r\n$ ctr         <dbl> 0.04566210, 0.04779412, 0.02955665, 0.12000000, ~\r\n$ position    <dbl> 3.713851, 3.669118, 5.211823, 3.470000, 4.687166~\r\n\r\nA tak dále. Výsledný dataframe vždy obsahuje dimenzi date, případné další dimenze a čtyři základní metriky, které Search Console eviduje.\r\nNekonzistence dat Search Console\r\nTeoreticky by stačilo stáhnout jenom poslední dataset sc_by_date_page_query, protože z něho by mělo jít všechno ostatní agregovat, jenže Search Console vrací různě upravené výsledky podle toho, zda si vyžádáte dimenze page nebo query, takže datasety bez těchto dimenzí mají přesnější čísla než data sety s nimi. Podrobněji to dokumentuju v tomhle testu.\r\nZ tohoto důvodu beru základní metriky (imprese, kliky, CTR a pozice) pokud možno jen z datasetu, který nebosahuje dimenze page nebo query. Datasety s těmito dimenzemi používám jen v případě, že potřebuju znát konkrétní stránky nebo jejich počty, resp. konkrétní dotazy nebo jejich počty.\r\nVizualizace časových řad\r\nZákladní metriky\r\nZákladní vizualizace, kterou skoro stejně ukazuje i webové rozhraní Search Console, vypadá takto:\r\n\r\n\r\nscale_factor <- max(sc_by_date$clicks) / max(sc_by_date$impressions)\r\nsc_by_date |>\r\n  ggplot(aes(x = date)) +\r\n  geom_line(aes(y = clicks), color = \"#4285f4\") +\r\n  geom_line(aes(y = impressions * scale_factor), color = \"#5e35b1\") +\r\n  scale_y_continuous(\r\n    name = \"clicks\",\r\n    sec.axis = sec_axis(~ . / scale_factor, name = \"impressions\")\r\n  ) +\r\n  theme(\r\n    axis.title.y.left = element_text(color = \"#4285f4\"),\r\n    axis.text.y.left = element_text(color = \"#4285f4\"),\r\n    axis.title.y.right = element_text(color = \"#5e35b1\"),\r\n    axis.text.y.right = element_text(color = \"#5e35b1\")\r\n  ) +\r\n  theme_minimal()\r\n\r\n\r\n\r\n\r\nTenhle graf má duální osu Y (čili dvě různé osy Y), což patří spíš do rejstříku „manažerských“ grafů, které se ve vážnější analytice z dobrých důvodů moc nepoužívájí. V R se místo toho většinou používají fasety (grafy vedle sebe a/nebo pod sebou), které nezkreslují měřítko, nepletou čtenáře co je co a navíc se do nich vejdou víc než dvě metriky, když je to potřeba.\r\nProtože budu v dalším kódu potřebovat podobné fasetové grafy často, připravím si na ně dvě funkce, abych nepsal opakovaně stejný kód.\r\nPrvní funkce sc_reshape provede tři věci:\r\nPřesune imprese před kliky, protože tohle pořadí mi připadá logičtější.\r\nPřevede širokou tabulku na dlouhou, tj. hodnoty metrik v druhém až posledním sloupci sloučí do jediného sloupce value (to je ve funkci pivot_longer default) a jejich názvy do sloupce metric.\r\nPřevede názvy metrik na typ factor, aby se fasety v grafu správně seřadily.\r\nK druhému bodu vysvětlení: Fasety v principu zobrazují jen jednu proměnnou ve více kategoriích, takže se tabulka musí předem převést na dlouhý formát, ve kterém se do jednoho sloupce dostanou názvy metrik jako kategorie a do druhého sloupce hodnoty všech metrik. K tomu slouží funkce pivot_longer.\r\n\r\n\r\nsc_reshape <- function(df) {\r\n  if (\"impressions\" %in% names(df)) {\r\n    df <- df |> \r\n      relocate(impressions, .before = clicks)\r\n  }\r\n  df |>\r\n    pivot_longer(cols = 2:last_col(), names_to = \"metric\") |>\r\n    mutate(metric = as_factor(metric))\r\n}\r\n\r\n\r\n\r\nDruhá funkce vykreslí samotný graf časových řad. Protože budu používat různé časové rozpětí a různé časové jednotky, je malinko složitější, aby se tomu přizpůsobila.\r\n\r\n\r\nsc_plot <- function(\r\n  df, x_breaks = \"3 months\", x_labels = \"%b %y\", x_minor_breaks = \"month\",\r\n  strip_pos = \"right\"\r\n) {\r\n  df |> \r\n    ggplot(aes_string(x = names(df[1]), y = \"value\")) +\r\n    geom_line() +\r\n    scale_x_date(\r\n      date_breaks = x_breaks, date_labels = x_labels,\r\n      date_minor_breaks = x_minor_breaks, expand = expansion(0.01)\r\n    ) +\r\n    facet_wrap(\r\n      ~ metric, ncol = 1, scales = \"free_y\", strip.position = strip_pos\r\n    ) +\r\n    theme_gray()\r\n}\r\n\r\n\r\n\r\nVe fasetách vypadají časové řady všech čtyř základních metrik takhle:\r\n\r\n\r\nsc_by_date |>\r\n  sc_reshape() |>\r\n  sc_plot()\r\n\r\n\r\n\r\n\r\nVýklad dat\r\nUž tenhle graf jde nějak interpretovat:\r\nNa začátku období neměl web z Googlu skoro žádnou návštěvnost, ale po cca třech měsících se začal postupně zlepšovat a zřejmě se zlepšuje pořád.\r\nCTR v prvním měsíci skákalo až k 50 %, což s nepatrným počtem impresí naznačuje, že web se v té době zobrazoval jen na extrémně nekonkurenční dotazy, které neměly ve výsledcích hledání jinou dobrou odpověď. To se postupně změnilo, CTR kleslo, a to naznačuje, že přibyly nějaké normálnější dotazy.\r\nTuto hypotézu podporuje i křivka průměrných pozic. Na začátku šíleně skáče, což je typické pro velmi malý počet dotazů, později se uklidní, takže se počet dotazů asi zvýšil.\r\nZ toho všecho bych usoudil, že web začínal s malým objemem obsahu a postupně se zvětšuje. Je to ale jen hypotéza, kterou je třeba podpořit dalšími daty.\r\nPočet stránek a dotazů\r\nAby šla data lépe interpretovat, skoro vždy přidávám ještě dvě další časové řady – počet vstupních stránek a počet unikátních dotazů. K tomu jsem si stáhl ze Search Sonsole dva další datasety, vstupní stránky podle dnů (sc_by_date_page) dotazy podle dnů (sc_by_date_query). Nyní z nich funkcemi group_by a summarise spočítám počty unikátních stránek resp. dotazů za den a funkcí left_join je připojím k původní tabulce metrik. Výsledek uložím do objektu sc_all_metrics_by_date.\r\n\r\n\r\nsc_all_metrics_by_date <- sc_by_date |>\r\n  left_join(\r\n    sc_by_date_page |>\r\n      group_by(date) |>\r\n      summarise(pages = n()),\r\n    by = \"date\"\r\n  ) |>\r\n  left_join(\r\n    sc_by_date_query |>\r\n      group_by(date) |>\r\n      summarise(queries = n()),\r\n    by = \"date\"\r\n  ) |>\r\n  mutate(\r\n    pages = replace_na(pages, 0),\r\n    queries = replace_na(queries, 0)\r\n  )\r\n\r\n\r\n\r\nVýslední tabulka vypadá takhle:\r\n\r\n\r\nglimpse(sc_all_metrics_by_date)\r\n\r\n\r\nRows: 498\r\nColumns: 7\r\n$ date        <date> 2020-11-28, 2020-11-29, 2020-11-30, 2020-12-01,~\r\n$ clicks      <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ~\r\n$ impressions <int> 0, 0, 0, 4, 5, 1, 0, 0, 1, 0, 2, 10, 4, 6, 3, 6,~\r\n$ ctr         <dbl> 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0~\r\n$ position    <dbl> 0.000000, 0.000000, 0.000000, 16.500000, 8.40000~\r\n$ pages       <int> 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, ~\r\n$ queries     <int> 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 2, 1, 2, 2, 1, ~\r\n\r\nGraf zobrazím stejně, jako ten se čtyřmi základními metrikami, jen funkcí scale_x_date trochu vyladím rozdělení osy X, aby bylo časové členění přehlednější.\r\n\r\n\r\nsc_all_metrics_by_date |>\r\n  sc_reshape() |>\r\n  sc_plot()\r\n\r\n\r\n\r\n\r\nMimochodem, obdobný graf jde odělat i v Data Studiu. Tady je příklad reportu, který si můžete zkopírovat a upravit pro své potřeby.\r\nZoomování grafu\r\nV druhé polovině grafu za celé období nejsou moc dobře vidět změny CTR a pozic, protože na začátku období mají tyto metriky příliš veliké rozpětí hodnot. Zazoomuju proto na období od začátku srpna 2021 prostým vyfiltrováním řádků, které do grafu vstupují.\r\n\r\n\r\nsc_all_metrics_by_date |>\r\n  filter(date >= as.Date(\"2021-08-01\")) |>\r\n  sc_reshape() |>\r\n  sc_plot(\"month\")\r\n\r\n\r\n\r\n\r\nGranularita období\r\nAž dosud jsem časové řady vykresloval po dnech. Pro data související s návštěvností webů je ale typická silná týdenní periodicita a v jejím důsledku grafy bývají hodně zubaté. Existuje několik způsobů, jak je vyhladit. Nejpřirozenější je snížit časovou granularitu a agregovat data za týden, měsíc, nebo ještě delší období. I to lze však udělat více způsoby, např mohu:\r\nSečíst to, co jde sečíst (imprese a kliky) a zprůměrovat to, co sčítat nejde (CTR a pozice) a počty unikátních položek (stránek a dotazů) spočítat znovu. To vše za celé nové časové jednotky, tedy např. za týdny.\r\nTotéž, ale klouzavě. Ke každému dni tedy totéž sečtu, zprůměruju, nebo znovu vypočtu za posledních sedm dní. Bude to sice výpočetně náročnější, protože místo jednou za týden to budu počítat za každý den, ale i když nebude celé období začínat a končit přesně na hranici týdnů (resp. měsíců či jiné časové jednotky), nebude graf na okrajích zkreslený.\r\nSpočítat prosté průměry či mediány všech hodnot za týden či jinou časovou jednotku.\r\nKromě průměru či mediánu spočítat a hezky vykreslit i pásmo, ve kterém se hodnoty pohybují.\r\nZkusím všechny varianty, ať je vidět, zda a případně jak se liší.\r\nAgregace za kalendářní období\r\nAgregace dat Search Console do delších časových jednotek, tedy typicky týdnů či měsíců případně kvartálů, není úplně triviální.\r\nImprese a kliky se normálně sečtou, tam problém není.\r\nCTR a pozice se musí zprůměrovat váženým průměrem, přičemž vahou jsou imprese. Není to úplně nezbytné, ale je to bezpečnější, protože prostý aritmetický průměr se od váženého někdy liší jen nepatrně a jindy (při velkém rozdílu impresí) hodně.\r\nPočty unikátních stránek a dotazů se musí spočítat funkcí n_distinct za každé dílčí období znovu. Rozdíl ve tvaru křivky sice nebývá moc veliký, ale zato je značný rozdíl ve velikosti hodnot – za den je unikátních dotazů či stránek podstatně méně než za týden nebo měsíc.\r\nPro usnadnění a zpřehlednění výpočtu si připravím několik funkcí.\r\nPrvní funkce agreguje tabulku základních metrik.\r\n\r\n\r\nsc_aggregate <- function(df, per = \"week\") {\r\n  df |>\r\n    group_by(\"{per}\" := floor_date(date, per)) |> \r\n    summarise(\r\n      across(ctr:position, ~ weighted.mean(.x, impressions)),\r\n      across(clicks:impressions, sum),\r\n    ) |> \r\n    relocate(ctr:position, .after = impressions)\r\n}\r\n\r\nsc_aggregate(sc_by_date, \"quarter\")\r\n\r\n\r\n\r\n\r\n{\"columns\":[{\"label\":[\"quarter\"],\"name\":[1],\"type\":[\"date\"],\"align\":[\"right\"]},{\"label\":[\"clicks\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"impressions\"],\"name\":[3],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"ctr\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"position\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"2020-10-01\",\"2\":\"4\",\"3\":\"89\",\"4\":\"0.04494382\",\"5\":\"9.191011\"},{\"1\":\"2021-01-01\",\"2\":\"80\",\"3\":\"2608\",\"4\":\"0.03067485\",\"5\":\"8.855061\"},{\"1\":\"2021-04-01\",\"2\":\"464\",\"3\":\"16597\",\"4\":\"0.02795686\",\"5\":\"6.794360\"},{\"1\":\"2021-07-01\",\"2\":\"1224\",\"3\":\"54718\",\"4\":\"0.02236924\",\"5\":\"7.350342\"},{\"1\":\"2021-10-01\",\"2\":\"2160\",\"3\":\"112025\",\"4\":\"0.01928141\",\"5\":\"7.175023\"},{\"1\":\"2022-01-01\",\"2\":\"2816\",\"3\":\"142337\",\"4\":\"0.01978403\",\"5\":\"6.819112\"},{\"1\":\"2022-04-01\",\"2\":\"357\",\"3\":\"17905\",\"4\":\"0.01993856\",\"5\":\"6.990896\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\r\n  \r\n\r\nDruhá funkce znovu spočítá počty unikátních stránek či dotazů. Předpokládá, že vstupní tabulka obsahuje sloupce date a sloupec řetězců, keré se mají spočítat. Na dalších sloupcích nezáleží.\r\n\r\n\r\nsc_recount <- function(df, per = \"month\", column, new_col) {\r\n  df |> \r\n    group_by(\"{per}\" := floor_date(date, per)) |> \r\n    summarise(\"{new_col}\" := n_distinct({{ column }}))\r\n}\r\n\r\nsc_recount(sc_by_date_page, \"quarter\", page, \"pages\")\r\n\r\n\r\n\r\n\r\n{\"columns\":[{\"label\":[\"quarter\"],\"name\":[1],\"type\":[\"date\"],\"align\":[\"right\"]},{\"label\":[\"pages\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"2020-10-01\",\"2\":\"3\"},{\"1\":\"2021-01-01\",\"2\":\"33\"},{\"1\":\"2021-04-01\",\"2\":\"78\"},{\"1\":\"2021-07-01\",\"2\":\"126\"},{\"1\":\"2021-10-01\",\"2\":\"141\"},{\"1\":\"2022-01-01\",\"2\":\"150\"},{\"1\":\"2022-04-01\",\"2\":\"126\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\r\n  \r\n\r\nA konečně mohu napsat poslední funkci, která sestaví všechny metriky do jedné tabulky.\r\n\r\n\r\nsc_aggregate_all_metrics <- function(per = \"week\") {\r\n  sc_aggregate(sc_by_date, per) |> \r\n    left_join(sc_recount(sc_by_date_page, per, page, \"pages\")) |> \r\n    left_join(sc_recount(sc_by_date_query, per, query, \"queries\")) |> \r\n    mutate(\r\n      pages = replace_na(pages, 0),\r\n      queries = replace_na(queries, 0)\r\n    )\r\n}\r\n\r\nsc_aggregate_all_metrics(\"quarter\")\r\n\r\n\r\n\r\n\r\n{\"columns\":[{\"label\":[\"quarter\"],\"name\":[1],\"type\":[\"date\"],\"align\":[\"right\"]},{\"label\":[\"clicks\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"impressions\"],\"name\":[3],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"ctr\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"position\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"pages\"],\"name\":[6],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"queries\"],\"name\":[7],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"2020-10-01\",\"2\":\"4\",\"3\":\"89\",\"4\":\"0.04494382\",\"5\":\"9.191011\",\"6\":\"3\",\"7\":\"5\"},{\"1\":\"2021-01-01\",\"2\":\"80\",\"3\":\"2608\",\"4\":\"0.03067485\",\"5\":\"8.855061\",\"6\":\"33\",\"7\":\"71\"},{\"1\":\"2021-04-01\",\"2\":\"464\",\"3\":\"16597\",\"4\":\"0.02795686\",\"5\":\"6.794360\",\"6\":\"78\",\"7\":\"345\"},{\"1\":\"2021-07-01\",\"2\":\"1224\",\"3\":\"54718\",\"4\":\"0.02236924\",\"5\":\"7.350342\",\"6\":\"126\",\"7\":\"1158\"},{\"1\":\"2021-10-01\",\"2\":\"2160\",\"3\":\"112025\",\"4\":\"0.01928141\",\"5\":\"7.175023\",\"6\":\"141\",\"7\":\"1923\"},{\"1\":\"2022-01-01\",\"2\":\"2816\",\"3\":\"142337\",\"4\":\"0.01978403\",\"5\":\"6.819112\",\"6\":\"150\",\"7\":\"2369\"},{\"1\":\"2022-04-01\",\"2\":\"357\",\"3\":\"17905\",\"4\":\"0.01993856\",\"5\":\"6.990896\",\"6\":\"126\",\"7\":\"868\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\r\n  \r\n\r\nAgregace po kalendářních týdnech\r\nS připravenými funkcemi již mohu snadno agregovat.\r\n\r\n\r\nsc_aggregate_all_metrics(\"week\") |> \r\n  filter(week >= floor_date(as.Date(\"2021-08-01\"), \"week\")) |>\r\n  sc_reshape() |>\r\n  sc_plot(\"4 weeks\", \"%x\", \"week\")\r\n\r\n\r\n\r\n\r\nAgregace po kalendářních měsících\r\n\r\n\r\nsc_aggregate_all_metrics(\"month\") |> \r\n  filter(\r\n    month >= floor_date(as.Date(\"2021-08-01\"), \"month\"),\r\n    month <= floor_date(as.Date(\"2022-03-01\"), \"month\")\r\n  ) |>\r\n  sc_reshape() |>\r\n  sc_plot(\"month\")\r\n\r\n\r\n\r\n\r\nPrůměr za kalendářní týdny\r\nPrůměr za kalendářní týdny je výpočetně mnohem jednodušší. Stačí spočítat průměry všech číselných hodnot. Jak ale píšu výše, pro CTR a pozice je metodicky nesprávný, a proto někdy méně a jindy více nepřesný, a u počtů stránek a dotazů sice metodicky dává smysl, nicméně říká něco jiného.\r\nKdyž si porovnáte tvar křivek metrik pages a queies, jsou sice podobné, ale čísla jsou úplně jiná – v prvním výpočtu (počet za týden) jsem se dostal na 110 stránek a 800 dotazů, kdežto v druhém (průměr týdenního počtu) jen na 80 resp. 300.\r\n\r\n\r\nsc_all_metrics_by_date |>\r\n  mutate(week = floor_date(date, \"week\")) |> \r\n  filter(week >= floor_date(as.Date(\"2021-08-01\"), \"week\")) |>\r\n  group_by(week) |> \r\n  summarise(across(clicks:queries, mean)) |> \r\n  sc_reshape() |> \r\n  sc_plot(\"4 weeks\", \"%x\", \"week\")\r\n\r\n\r\n\r\n\r\nRozsah hodnot za týden\r\nNa tomto grafu vykreslím medián jako čáru, mezikvartilové rozpětí (interquartile range, tj. rozsah mezi 1. a 3. kvartilem) jako tmavší pás a variační rozpětí (rozdíl mezi maximem a minimem) jako světlejší pás.\r\n\r\n\r\nsc_all_metrics_by_date |>\r\n  mutate(week = floor_date(date, \"week\"), .before = date) |> \r\n  filter(week >= floor_date(as.Date(\"2021-08-01\"), \"week\")) |> \r\n  select(!date) |> \r\n  sc_reshape() |> \r\n  group_by(week, metric) |> \r\n  summarise(\r\n    median = median(value),\r\n    lower = quantile(value, probs = 0.25),\r\n    upper = quantile(value, probs = 0.75),\r\n    min = min(value),\r\n    max = max(value),\r\n    .groups = \"drop\"\r\n  ) |> \r\n  ggplot(aes(x = week)) +\r\n  geom_ribbon(aes(ymin = min, ymax = max), fill = \"gray80\") +\r\n  geom_ribbon(aes(ymin = lower, ymax = upper), fill = \"gray60\") +\r\n  geom_line(aes(y = median), color = \"gray25\") +\r\n  scale_x_date(\r\n    date_breaks = \"4 weeks\", date_labels = \"%x\", date_minor_breaks = \"week\",\r\n    expand = expansion(0.025)\r\n  ) +\r\n  facet_wrap(~metric, ncol = 1, scales = \"free_y\", strip.position = \"right\")\r\n\r\n\r\n\r\n\r\nKlouzavý průměr (moving average)\r\nKlouzavý průměr za posledních sedm dní zpopularizovaly různé covidové statistiky, ale pro data Search Console obvykle nemá smysl. Hodí se jen v případě, že potřebujete znát co nejnovější hodnotu (typicky včerejší) očištěnou od týdenní sezónnosti, přestože týden ještě neskončil.\r\nPro jistotu ukážu, jak jde klouzavý průměr za posledních 7 dní spočítat (zde jen imprese), přestože ho sám nepoužívám. Pod něj zároveň šedou barvou vykreslím skutečné imprese po dnech.\r\n\r\n\r\nsc_by_date |>\r\n  mutate(\r\n    impressions_avg7 = slider::slide_dbl(impressions, ~ mean(.x), .before = 6)\r\n  ) |>\r\n  ggplot(aes(x = date)) +\r\n  geom_line(aes(y = impressions), color = \"gray\") +\r\n  geom_line(aes(y = impressions_avg7)) +\r\n  scale_x_date(\r\n    date_breaks = \"3 months\", date_labels = \"%b %y\", \r\n    date_minor_breaks = \"month\", expand = expansion(0.01)\r\n  )\r\n\r\n\r\n\r\n\r\nRegrese\r\nV praxi je úplně nejjednodušší vyhledit denní křivku nějakou regresí. V balíčku ggplot2 je na to hotový geom_smooth.\r\n\r\n\r\nsc_by_date |>\r\n  sc_reshape() |>\r\n  sc_plot() +\r\n  geom_smooth()\r\n\r\n\r\n\r\n\r\nPokud chci křivku klikatější, jde to nastavit parametrem span.\r\n\r\n\r\nsc_by_date |>\r\n  sc_reshape() |>\r\n  sc_plot() +\r\n  geom_smooth(span = 0.1)\r\n\r\n\r\n\r\n\r\nA ještě ukážu, jak jde geom_smooth vykreslit na jediné metrice za kratší období, protože to je typ grafu, který asi používám v různých reportech pro klienty nejčastěji. Šedý pás okolo modré křivky je konfidenční interval.\r\n\r\n\r\nsc_by_date |>\r\n  filter(date >= as.Date(\"2022-01-01\")) |> \r\n  ggplot(aes(x = date, y = impressions)) +\r\n  geom_line(color = \"gray50\") +\r\n  geom_smooth() +\r\n  scale_x_date(date_labels = \"%b %y\")\r\n\r\n\r\n\r\n\r\nPoměrové metriky\r\nZ předešlých grafů už vím, že imprese i kliky dlouhodobě rostou a že to je mj. dáno tím, že roste počet vstupních stránek a dotazů. Tahle informace mi ale nestačí. Existují totiž tři možné varianty:\r\nImprese a kliky rostou víc, než by odpovídalo nárůstu počtu stránek a dotazů. To by se mi líbilo nejvíc, protože by to znamenalo, že novější stránky resp. dotazy jsou úspěšnější než starší, případně se na růstu impresí a kliků podílí ještě něco jiného, např. celková síla webu.\r\nRúst impresí a kliků zhruba odpovídá růstu počtu stránek a dotazů. To je neutrální výsledek – odměna za vynaloženou práci na obsahu (a tedy efektivita této práce) zůstává pořád zhruba stejná, ani neklesá, ani se nezvyšuje.\r\nPoslední varianta je nejhorší: imprese a kliky rostou méně, než by odpovídalo nárůstu počtu stránek a dotazů. Efektivita vložené práce se tedy zhoršuje. Někdy to jinak nejde, protože nízko visíci ovoce je už otrhané, ale je dobré o tom vědět.\r\nNa tyhle otázky pomáhají najít odpověď poměrové metriky. V principu jde o poměry počtu impresí a kliků na jedné straně a počtu stránek a dotazů na straně druhé. Navíc je zajímavý i poměr počtu dotazů na stránku.\r\n\r\n\r\nsc_aggregate_all_metrics() |> \r\n  filter(week >= floor_date(as.Date(\"2021-08-01\"), \"week\")) |> \r\n  transmute(\r\n    week = week,\r\n    impressions_per_page = impressions / pages,\r\n    clicks_per_page = clicks / pages,\r\n    impressions_per_query = impressions / queries,\r\n    clicks_per_query = clicks / queries,\r\n    queries_per_page = queries / pages\r\n  ) |>\r\n  sc_reshape() |> \r\n  sc_plot(\"4 weeks\", \"%x\", \"week\", strip_pos = \"top\")\r\n\r\n\r\n\r\n\r\nZ grafu je vidět, že počty impresí a kliků na stránku stále rostou, což je dobře – odpovídá to variantě 1. S impresemi a kliky na dotaz je to už slabší, ty posledních cca 5 měsíců stagnují. Co to znamená? Na to odpovídá poslední poměrová metrika, počet dotazů na stránku. Ta roste, což znamená, že novější stránky jsou efektivnější především proto, že pokrývají víc dotazů, avšak jednotlivé novéjší dotazy jsou efektivní zhruba stejně.\r\nK tomu jen poznámka: Grafy samy o sobě nejsou důkazem tvrzení, která zde píšu. Jsou jen dobrým podkladem k odbornému odhadu, který je tím spolehlivější, čím lépe znám daný web a jeho historii.\r\nMetriky podle pozic\r\nOblíbené vizualizace jsou různé metriky podle pozic nebo intervalů pozic. Např. Collabim a Marketing Miner nabízí graf počtu dotazů podle skupin pozic. I takové grafy jsou ze Search Console udělat.\r\nPočet dotazů podle intervalů pozic\r\nV Collabimu a Marketing Mineru je 100% plošný graf počtu dotazů podle intervalů pozic. Ten v principu vypadá takhle, byť má intervaly pozic nastavené jinak:\r\n\r\n\r\nsc_by_date_query |>\r\n  filter(date >= as.Date(\"2021-08-01\")) |> \r\n  mutate(\r\n    pos_int = cut(\r\n      position,\r\n      breaks = c(10 * 0:3, Inf),\r\n      labels = c(\"1-10\", \"11-20\", \"21-30\", \"31+\")\r\n    )\r\n  ) |>\r\n  group_by(date, pos_int) |>\r\n  summarise(queries = n_distinct(query)) |>\r\n  mutate(queries = queries / sum(queries)) |> \r\n  ungroup() |> \r\n  ggplot(aes(x = date, y = queries, fill = pos_int)) +\r\n  geom_area() +\r\n  scale_x_date(\r\n    date_breaks = \"3 months\", date_labels = \"%b %y\", \r\n    date_minor_breaks = \"month\", expand = expansion(0.01)\r\n  ) +\r\n  scale_fill_brewer(type = \"seq\", direction = -1)\r\n\r\n\r\n\r\n\r\nTenhle graf má ale význam jen pro pevně danou sadu dotazů, která se v čase nemění. V kontextu Search Console, kde se dotazy a jejich počet každý den mění, nedává příliš smysl. O trochu užitečnější je graf skutečných počtů dotazů nepřepočítaných na procenta. Z toho je alespoň vidět, že počet dotazů roste a růst táhnou dotazy v první desítce.\r\n\r\n\r\nsc_by_date_query |>\r\n  filter(date >= as.Date(\"2021-08-01\")) |> \r\n  mutate(\r\n    pos_int = cut(\r\n      position,\r\n      breaks = c(10 * 0:3, Inf),\r\n      labels = c(\"1-10\", \"11-20\", \"21-30\", \"31+\")\r\n    )\r\n  ) |>\r\n  group_by(date, pos_int) |>\r\n  summarise(queries = n_distinct(query), .groups = \"drop\") |>\r\n  ggplot(aes(x = date, y = queries, fill = pos_int)) +\r\n  geom_area() +\r\n  scale_x_date(\r\n    date_breaks = \"3 months\", date_labels = \"%b %y\", \r\n    date_minor_breaks = \"month\", expand = expansion(0.01)\r\n  ) +\r\n  scale_fill_brewer(type = \"seq\", direction = -1)\r\n\r\n\r\n\r\n\r\nPočet vstupních stránek podle intervalů pozic\r\nObdobný graf lze vykreslit i pro počty vstupních stránek\r\n\r\n\r\nsc_by_date_page |>\r\n  filter(date >= as.Date(\"2021-08-01\")) |> \r\n  mutate(\r\n    pos_int = cut(\r\n      position,\r\n      breaks = c(10 * 0:3, Inf),\r\n      labels = c(\"1-10\", \"11-20\", \"21-30\", \"31+\")\r\n    )\r\n  ) |>\r\n  group_by(date, pos_int) |>\r\n  summarise(pages = n_distinct(page), .groups = \"drop\") |>\r\n  ggplot(aes(x = date, y = pages, fill = pos_int)) +\r\n  geom_area() +\r\n  scale_x_date(\r\n    date_breaks = \"3 months\", date_labels = \"%b %y\", \r\n    date_minor_breaks = \"month\", expand = expansion(0.01)\r\n  ) +\r\n  scale_fill_brewer(type = \"seq\", direction = -1)\r\n\r\n\r\n\r\n\r\nImprese podle intervalů pozic\r\nGraf jde vykreslit i pro imprese nebo kliky (zde imprese). Zároveň jsem mu nastavil i jiné intervaly pozic, aby bylo vidět pordobnější rozdělení pozic na první stránce výsledků hledání.\r\nAle upřímně, všechny tyhle grafy mají z dat Search Console málokdy smysl, protože bývají pro většinu webů hodně podobné a nějaký důležitý vhled z nich získám jen výjimečně.\r\n\r\n\r\nsc_by_date_query |>\r\n  filter(date >= as.Date(\"2021-08-01\")) |> \r\n  mutate(\r\n    pos_int = cut(\r\n      position,\r\n      breaks = c(1, 1.5, 3.5, 5.5, 10.01, Inf),\r\n      labels = c(\"1\", \"2-3\", \"4-5\", \"6-10\", \"10+\"),\r\n      right = FALSE\r\n    )\r\n  ) |>\r\n  group_by(date, pos_int) |>\r\n  summarise(impressions = sum(impressions), .groups = \"drop\") |>\r\n  ggplot(aes(x = date, y = impressions, fill = pos_int)) +\r\n  geom_area() +\r\n  scale_x_date(\r\n    date_breaks = \"3 months\", date_labels = \"%b %y\", \r\n    date_minor_breaks = \"month\", expand = expansion(0.01)\r\n  ) +\r\n  scale_fill_brewer(type = \"seq\", direction = -1)\r\n\r\n\r\n\r\n\r\nPorovnání metrik s předešlým obdobím nebo rokem\r\nDalším populárním grafem je porovnání běžného období s obdobím před rokem (YoY, year over year) nebo s obdobím bezprostředně předcházejícím.\r\nMeziroční porovnání (YoY)\r\nPrincip přípravy je jednoduchý. Vím, že mám v dlouhé tabulce 6 metrik, takže ke každému řádku přidám funkcí lag hodnotu z řádku o 6 * 365 řádků zpátky. Tuhle hodnotu pak vykreslím tečkovanou čárou.\r\n\r\n\r\nsc_all_metrics_by_date |> \r\n  sc_reshape() |> \r\n  mutate(value_prev = lag(value, 6 * 365)) |> \r\n  filter(date >= as.Date(\"2022-01-01\")) |>\r\n  sc_plot(\"month\") +\r\n  geom_line(aes(y = value_prev), linetype = \"dotted\")\r\n\r\n\r\n\r\n\r\nPosledních 12 týdnů a 12 týdnů před tím\r\nObdobně jde vykreslit jakékoli období s jakoukoli granularitou a k němu stejně dlouhé období bezprostředně předcházející.\r\n\r\n\r\nsc_aggregate_all_metrics(\"week\") |> \r\n  sc_reshape() |> \r\n  mutate(value_prev = lag(value, 6 * 12)) |> \r\n  filter(week >= floor_date(as.Date(\"2022-01-03\"), \"week\")) |> \r\n  sc_plot(\"2 weeks\", \"%x\", \"week\") +\r\n  geom_line(aes(y = value_prev), linetype = \"dotted\")\r\n\r\n\r\n\r\n\r\nAutokorelace\r\nZajímavá varianta je, když se vykreslí jen rozdíly (případně podíly resp. procenta) mezi aktuální a předešlou hodnotou. V principu se jedná o svislou úsečku mezi plnou a tečkovanou z grafů výše, která má základ v nule.\r\nZde je příklad posledních 12 týdnů v porovnání s předešlými 12 týdny vykreslený po dnech. Ukazuje, o kolik byla daná metrika v daný den vyšší než před 12 týdny.\r\n\r\n\r\nsc_all_metrics_by_date |>\r\n  sc_reshape() |> \r\n  mutate(value = value - lag(value, 6 * 7 * 12)) |> \r\n  filter(date >= as.Date(\"2022-01-01\")) |>\r\n  ggplot(aes(x = date, y = value)) +\r\n  geom_segment(aes(xend = date, yend = 0)) +\r\n  scale_x_date(date_breaks = \"month\", date_labels = \"%b %y\") +\r\n  facet_wrap(~metric, ncol = 1, scales = \"free_y\", strip.position = \"right\")\r\n\r\n\r\n\r\n\r\nDny v týdnu\r\nPro většinu webů je typická týdenní periodicita. Někde je slabší, někde silnější a liší se i její průběh. Asi nejlepším porovnáním je boxplot, který ukazuje jednak medián (tlustší vodorovná čára uvnitř boxu) a jednak i celou distribuci hodnt (box reprezentuje 50 % hodnot uprostřed, box i s fousy rozpětí bez odlehlých hodnot, které reprezentují puntíky).\r\nGraf má smysl spíš jen pro imprese a kliky, protože u ostatních metrik se periodicita moc neprojevuje.\r\n\r\n\r\nsc_by_date |>\r\n  filter(date >= as.Date(\"2021-08-01\")) |>\r\n  select(!c(ctr, position)) |> \r\n  sc_reshape() |> \r\n  mutate(day = wday(date, label = TRUE, week_start = 1)) |> \r\n  ggplot(aes(x = day, y = value)) +\r\n  geom_boxplot() +\r\n  facet_wrap(~metric, ncol = 1, scales = \"free_y\", strip.position = \"right\")\r\n\r\n\r\n\r\n\r\nZ grafu je vidět, že zrovna tenhle web silnou týdenní periodicitu nemá. Sice od pondělí trochu vystoupá do úterý, pak až do soboty mírně klesá a znovu stoupne v neděli, ale rozdíly hodnot nejsou moc přesvědčivé.\r\nDekompozice časové řady\r\nPro dekompozici časových řad na trend, sezónnost a šum existuje mnoho teoretických modelů a balíčků, které je počítají. Pro příklad ukážu postup s balíčkem feasts z rodiny balíčku tsibble. Ten sice nějakou sezónnost našel, ale zároveň ukazuje, že se pohybuje jen v rozpětí cca 100 impresí (-50 až 50), což v objemu nějakých 2000 impresí za den není zrovna moc. Navíc je vidět, že rozpětí šumu (označený jako random) je čtyřikrát větší.\r\n\r\n\r\nsc_by_date |>\r\n  filter(date >= as.Date(\"2022-01-01\")) |> \r\n  as_tsibble(index = date) |>\r\n  fabletools::model(feasts::classical_decomposition(impressions)) |>\r\n  fabletools::components() |>\r\n  autoplot()\r\n\r\n\r\n\r\n\r\nNutno poznamenat, že sezónnost v v rámci roku nejde z dat Search Console odvodit, protože je k dispozici jen 16 měsíců dat. Museli byste si data sami archivovat za delší období.\r\nSegmentace podle URL\r\nData Search Console se nejčastěji segmentují podle skupin vstupních stránek (URL) nebo podle skupin dotazů. Většinou segmentuju aktuální stav (tj. např. posledních 28 dní) sloupcovým grafem, ale vývoj v čase se občas hodí taky.\r\nV tomto konkrétním případě se v URL hned za doménou nachází složka representující kategorii (rubriku či sekci webu), takže se nabízí segmentace stránek podle těchto kategorií. Dále jde segmentovat i podle typu stránky – na webu jsou jednak stránky rubrik se seznamem článků (ty mají za doménou jen jedno lomítko) a jednak články, které mají za doménou dvě lomítka.\r\nTyp stránky: rubriky vs. články\r\nVyjdu z datasetu sc_by_date_page a doplním ho o sloupec reprezentující typ stránky. Imprese a kliky sečtu za měsíc a rubriku a přidám i počet unikátních stránek. Výsledek vykreslím jako vrstvený plošný graf (stacked area chart). Rubriku poznám tak, že má v URL celkem 3 lomítka, kdežto článek má 4. Tím se mi do rubrik dostane i homepage, ale to mi nevadí.\r\n\r\n\r\nsc_by_date_page |> \r\n  filter(\r\n    between(date, as.Date(\"2021-01-01\"), as.Date(\"2022-03-31\")),\r\n  ) |> \r\n  mutate(\r\n    page_type = if_else(str_count(page, \"/\") == 3, \"rubrika\", \"článek\"),\r\n    month = floor_date(date, \"month\")\r\n  ) |> \r\n  group_by(month, page_type) |> \r\n  summarise(\r\n    n = n_distinct(page),\r\n    impressions = sum(impressions),\r\n    clicks = sum(clicks),\r\n    .groups = \"drop\"\r\n  ) |> \r\n  pivot_longer(cols = n:last_col(), names_to = \"metric\") |> \r\n  ggplot(aes(x = month, y = value, fill = page_type)) +\r\n  geom_area() +\r\n  facet_wrap(~ metric, ncol = 1, scales = \"free_y\", strip.position = \"right\") +\r\n  scale_fill_brewer(type = \"qual\", palette = 1)\r\n\r\n\r\n\r\n\r\nVýsledek je jednoznačný: rubrik je oproti článkům velmi málo a na viditelnosti nebo návštěvnosti z Googlu se reálně nepodílejí.\r\nRubriky\r\nRubriky identifikuje první složka za doménou, takže ji stačí z URL vykousnout funkcí str_extract z balíčku stringr, které pošlu jen cestu za doménou získanou funkcí path z balíčku urltools. Homepage vyřadím (má ve sloupci category hodnotu NA).\r\n\r\n\r\nsc_by_date_page |> \r\n  mutate(\r\n    category = str_extract(path(page), \"^[^/]+\"),\r\n    month = floor_date(date, \"month\")\r\n  ) |> \r\n  filter(\r\n    between(month, as.Date(\"2021-01-01\"), as.Date(\"2022-03-31\")),\r\n    !is.na(category)\r\n  ) |> \r\n  group_by(month, category) |> \r\n  summarise(\r\n    n = n_distinct(page),\r\n    impressions = sum(impressions),\r\n    clicks = sum(clicks),\r\n    .groups = \"drop\"\r\n  ) |> \r\n  pivot_longer(cols = n:last_col(), names_to = \"metric\") |> \r\n  ggplot(aes(x = month, y = value, fill = category)) +\r\n  geom_area() +\r\n  facet_wrap(~ metric, ncol = 1, scales = \"free_y\", strip.position = \"right\") +\r\n  scale_fill_brewer(type = \"qual\", palette = 8)\r\n\r\n\r\n\r\n\r\nTady už je interpretace zajímavější.\r\nStránek v rubrice značky je hodně, ale impresí a kliků mají málo. Lze je tedy považovat za nepříliš úspěšné. Obdobně je na tom katalog.\r\nOpakem jsou trendy, které mají velmi málo stránek, ale získávají hodně impresí i kliků.\r\nRubriky nákupy a materiály mají hodně impresí, ale z hlediska viditelnosti nákupy fungují efektivněji, protože impresí dosahují s menším počtem stránek. Zase ale mají horší CTR.\r\nZajímavá je rubrika m (magazín), která má hodně kliků při menším počtu impresí (takže vysoké CTR) a navíc jí kliky v posledních měsících rychle rostou i při stagnujícím počtu stránek.\r\nAby byla efektivita výkonových metrik (kliky a imprese) vzhledem k počtu unikátních stránek vidět ještě lépe, lze použít poměrové metriky: počet impresí na stránku a počet kliků na stránku.\r\n\r\n\r\nsc_by_date_page |> \r\n  mutate(\r\n    category = str_extract(path(page), \"^[^/]+\"),\r\n    month = floor_date(date, \"month\")\r\n  ) |> \r\n  filter(\r\n    between(month, as.Date(\"2021-01-01\"), as.Date(\"2022-03-31\")),\r\n    !is.na(category)\r\n  ) |> \r\n  group_by(month, category) |> \r\n  summarise(\r\n    impressions_pp = sum(impressions) / n_distinct(page),\r\n    clicks_pp = sum(clicks) / n_distinct(page),\r\n    .groups = \"drop\"\r\n  ) |> \r\n  pivot_longer(cols = impressions_pp:last_col(), names_to = \"metric\") |> \r\n  ggplot(aes(x = month, y = value, fill = category)) +\r\n  geom_area() +\r\n  facet_wrap(~ metric, ncol = 1, scales = \"free_y\", strip.position = \"right\") +\r\n  scale_fill_brewer(type = \"qual\", palette = 8)\r\n\r\n\r\n\r\n\r\nZde je ještě lépe vidět malá efektivita značek a katalogu a vysoká efektivita trendů.\r\nZ těchto poznatků jde odvodit docela dost doporučení pro další práci s obsahem, ale to již záleží na konkrétním kontextu projektu.\r\nPodobně jde segmentovat podle dotazů. Nejčastěji se tak rozlišuje brandové hledání od obecného, ale jde zapojit i klasifikace dotazů z analýzy klíčových slov a segementovat podle ní. To už ale přesahuje téma tohoto článku.\r\nA to je všechno :-)\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-04-12-search-console-timeline/search-console-timeline_files/figure-html5/unnamed-chunk-29-1.png",
    "last_modified": "2022-04-19T09:40:16+02:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 780
  },
  {
    "path": "posts/2022-04-09-cistim-search-consoli/",
    "title": "Čístím Search Consoli od zbytečných webů",
    "description": "Jako každý SEO konzultantant mám v Google Search Consoli plno webů, ke kterým už nemám přístup, nebo které už dlouho nefungují. Napsal jsem si skript v R, který je odstraní.",
    "author": [
      {
        "name": "Marek Prokop",
        "url": {}
      }
    ],
    "date": "2022-04-10",
    "categories": [],
    "contents": "\r\nPříprava\r\nSkriptu stačí tyhle dva balíčky.\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(searchConsoleR)\r\n\r\n\r\n\r\nA klasická autorizace do Search Console. Kdybych neměl svůj Google e-mail uložený v systémové proměnné, musel bych ho napsat do parametru email přímo.\r\n\r\n\r\nscr_auth(email = Sys.getenv(\"MY_GOOGLE_ACCOUNT\"))\r\n\r\n\r\n\r\nFunkcí list_websites načtu všechny weby, které v Search Consoli mám, a uložím si je do objektu websites.\r\n\r\n\r\nwebsites <- list_websites()\r\n\r\n\r\n\r\nSeznam vypadá nějak takhle, jen jsem konkrétní weby ze své Search Console anonymizoval, abych mohl výstup publikovat. Když vynechám poslední řádek, budou weby normálně vidět.\r\n\r\n\r\nwebsites |> \r\n  slice_sample(n = 10) |> \r\n  mutate(siteUrl = anonymize_site(siteUrl))\r\n\r\n\r\n\r\n\r\n{\"columns\":[{\"label\":[\"siteUrl\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"permissionLevel\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"sc-domain:anonymized.cz\",\"2\":\"siteFullUser\"},{\"1\":\"https://anonymized.eu/\",\"2\":\"siteFullUser\"},{\"1\":\"https://anonymized.cz/\",\"2\":\"siteRestrictedUser\"},{\"1\":\"https://anonymized.cz/\",\"2\":\"siteFullUser\"},{\"1\":\"https://anonymized.se/\",\"2\":\"siteFullUser\"},{\"1\":\"http://anonymized.cz/\",\"2\":\"siteRestrictedUser\"},{\"1\":\"sc-domain:anonymized.com\",\"2\":\"siteOwner\"},{\"1\":\"http://anonymized.cz/\",\"2\":\"siteOwner\"},{\"1\":\"sc-domain:anonymized.uk\",\"2\":\"siteFullUser\"},{\"1\":\"https://anonymized.sk/\",\"2\":\"siteFullUser\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\r\n  \r\n\r\nNeautorizované weby\r\nDůležitý je sloupec permissionLevel. Mám v něm tyhle hodnoty:\r\n\r\n\r\nwebsites |> \r\n  count(permissionLevel)\r\n\r\n\r\n\r\n\r\n{\"columns\":[{\"label\":[\"permissionLevel\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"n\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"siteFullUser\",\"2\":\"61\"},{\"1\":\"siteOwner\",\"2\":\"31\"},{\"1\":\"siteRestrictedUser\",\"2\":\"9\"},{\"1\":\"siteUnverifiedUser\",\"2\":\"15\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\r\n  \r\n\r\nZbavit se chci webů, ke kterým nemám povolený přístup. Vypíšu si je a raději je pečlivě zkontroluju (samozřejmě až odstraním anonymizaci na posledním řádku).\r\n\r\n\r\nunverified_websites <- websites |> \r\n  filter(permissionLevel == \"siteUnverifiedUser\")\r\n\r\nunverified_websites |> \r\n  mutate(siteUrl = anonymize_site(siteUrl))\r\n\r\n\r\n\r\n\r\n{\"columns\":[{\"label\":[\"siteUrl\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"permissionLevel\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"sc-domain:anonymized.cz\",\"2\":\"siteUnverifiedUser\"},{\"1\":\"https://anonymized.cz/\",\"2\":\"siteUnverifiedUser\"},{\"1\":\"https://anonymized.cz/\",\"2\":\"siteUnverifiedUser\"},{\"1\":\"https://anonymized.cz/\",\"2\":\"siteUnverifiedUser\"},{\"1\":\"https://anonymized.cz/\",\"2\":\"siteUnverifiedUser\"},{\"1\":\"http://anonymized.com/\",\"2\":\"siteUnverifiedUser\"},{\"1\":\"https://anonymized.cz/\",\"2\":\"siteUnverifiedUser\"},{\"1\":\"https://anonymized.cz/\",\"2\":\"siteUnverifiedUser\"},{\"1\":\"https://anonymized.com/\",\"2\":\"siteUnverifiedUser\"},{\"1\":\"http://anonymized.cz/\",\"2\":\"siteUnverifiedUser\"},{\"1\":\"http://anonymized.cz/\",\"2\":\"siteUnverifiedUser\"},{\"1\":\"https://anonymized.cz/\",\"2\":\"siteUnverifiedUser\"},{\"1\":\"https://anonymized.org/\",\"2\":\"siteUnverifiedUser\"},{\"1\":\"http://anonymized.cz/\",\"2\":\"siteUnverifiedUser\"},{\"1\":\"http://anonymized.cz/\",\"2\":\"siteUnverifiedUser\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\r\n  \r\n\r\nZe Search Console si je pak odstraním takto. Jen před tím přepíšu na prvním řádku FALSE na TRUE. Dal jsem to tam proto, abych si nechtěně nesmazal weby před tím, než si je zkontroluju.\r\n\r\n\r\nif (FALSE) {\r\n  unverified_websites |> \r\n    pull(siteUrl) |> \r\n    walk(delete_website)\r\n}\r\n\r\n\r\n\r\nKdyž to pustím, křičí to na mě nějaké warningy ohledně JSON. Asi je někde nějaká chybka, ale podle všeho ničemu nevadí a skript dělá to, co má. Každopádně si pak můžu ověřit, jestli už jsou všechny neverifikované weby pryč:\r\n\r\n\r\nlist_websites() |> \r\n  filter(permissionLevel == \"siteUnverifiedUser\")\r\n\r\n\r\n\r\nWeby bez dat\r\nKromě neutorizovaných webů se chci zbavit i těch, které už dlouho nemají žádná data.\r\nNejdřív si definuju funkci, která načte souhrnné metriky jednoho webu za poslední dva roky. K výsledku přidám do prvního sloupce i URL webu, abych ho poznal, až jich bude v tabulce víc.\r\n\r\n\r\nget_site_metrics <- function(site) {\r\n  search_analytics(\r\n    siteURL = site, \r\n    startDate = Sys.Date() - 365 * 2, \r\n    endDate = Sys.Date()\r\n  ) |> \r\n    add_column(site = site, .before = 1)\r\n}\r\n\r\n\r\n\r\nOvěřím, zda funkce funguje.\r\n\r\n\r\nget_site_metrics(\"http://www.marekp.cz/\")\r\n\r\n\r\n\r\n\r\n{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"site\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"clicks\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"impressions\"],\"name\":[3],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"ctr\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"position\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"http://www.marekp.cz/\",\"2\":\"2837\",\"3\":\"51402\",\"4\":\"0.0551924\",\"5\":\"9.855045\",\"_rn_\":\"1\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\r\n  \r\n\r\nFunguje, takže ji mohu pomocí funkce map_dfr z balíčku purrr aplikovat na celý seznam webů, ke kterým mám autorizovaný přístup. S více weby to nějakou chvíli poběží.\r\n\r\n\r\nall_site_metrics <- websites |> \r\n  filter(permissionLevel != \"siteUnverifiedUser\") |> \r\n  pull(siteUrl) |> \r\n  map_dfr(get_site_metrics)\r\n\r\n\r\n\r\nA teď už jen vyfiltruju a zobrazím (opět anonymizovaně) weby, které nemají žádné imprese.\r\n\r\n\r\nall_site_metrics |> \r\n  filter(impressions == 0) |> \r\n  mutate(site = anonymize_site(site))\r\n\r\n\r\n\r\n\r\n{\"columns\":[{\"label\":[\"site\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"clicks\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"impressions\"],\"name\":[3],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"ctr\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"position\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"https://anonymized.cz/\",\"2\":\"0\",\"3\":\"0\",\"4\":\"0\",\"5\":\"0\"},{\"1\":\"https://anonymized.com/\",\"2\":\"0\",\"3\":\"0\",\"4\":\"0\",\"5\":\"0\"},{\"1\":\"http://anonymized.com/\",\"2\":\"0\",\"3\":\"0\",\"4\":\"0\",\"5\":\"0\"},{\"1\":\"https://anonymized.cz/\",\"2\":\"0\",\"3\":\"0\",\"4\":\"0\",\"5\":\"0\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\r\n  \r\n\r\nPokud je chci ze Search Console odebrat, udělám to podle návodu pro neautorizované weby výše.\r\nA to je všechno :-)\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-04-10T11:12:51+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-04-09-kontrola-http-kodu/",
    "title": "Kontrola stavových kódů většího počtu URL v R",
    "description": "Potřeboval jsem rychle zkontrolovat stavové kódy HTTP dlouhého seznamu URL. Přitom jsem si vyzkoušel balíček furrr na paralelní zpracování a moc se mi zalíbil.",
    "author": [
      {
        "name": "Marek Prokop",
        "url": {}
      }
    ],
    "date": "2022-04-09",
    "categories": [],
    "contents": "\r\nKlient nedávno měnil CMS i doménu webu, došlo ke změně hodně URL, klasický zmatek, jako v těhle případech vždy. Asi měsíc po změně se ukázalo, že se něco nepovedlo a URL starého webu, které měly být přesměrované na nový web, nyní vrací chybu. Navíc jsou některé z nich ještě stále ve výsledcích hledání Googlu.\r\nVytáhli jsme tedy ze Search Console starého webu URL všech stránek, které se od změny domény alespoň jednou zobrazily, a mým úkolem bylo rychle zkontrolovat, jaké HTTP kódy vracejí. Konkrétně mě zajímalo, která URL fungují (vrací HTTP 200), neexistují (vrací chybu 404 nebo jinou 4xx), nebo na serveru způsobí nějakou chybu (kódy 5xx).\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(furrr)\r\nlibrary(rvest)\r\nlibrary(httr)\r\nlibrary(tictoc)\r\n\r\n\r\n\r\nPro potřeby tohoto zápisku jsem skutečná URL nahradil odkazy z úvodní stránky Wikipedie, ke kterým jsem navíc přidal 10 náhodných adres, aby mi to ukázalo nějaké chyby. Na principu to nic nemění. Ty odkazy jsem získal takhle:\r\n\r\n\r\nstart_url <- \"https://www.wikipedia.org/\"\r\n\r\nurls <- read_html(start_url) |>\r\n  html_elements(\"a\") |>\r\n  html_attr(\"href\") |>\r\n  xml2::url_absolute(start_url) |> \r\n  c(paste0(start_url, stringi::stri_rand_strings(10, 15)))\r\n\r\n\r\n\r\nJedná se o 337 adres, a náhodný vzorek deseti z nich vypadá takhle:\r\n\r\n\r\nsample(urls, 10)\r\n\r\n\r\n [1] \"https://pag.wikipedia.org/\"               \r\n [2] \"https://www.wikipedia.org/tubP9vI3wi8YxaP\"\r\n [3] \"https://ak.wikipedia.org/\"                \r\n [4] \"https://ht.wikipedia.org/\"                \r\n [5] \"https://ilo.wikipedia.org/\"               \r\n [6] \"https://si.wikipedia.org/\"                \r\n [7] \"https://su.wikipedia.org/\"                \r\n [8] \"https://gd.wikipedia.org/\"                \r\n [9] \"https://trv.wikipedia.org/\"               \r\n[10] \"https://vec.wikipedia.org/\"               \r\n\r\nKontrola jednoho URL\r\nPro kontrolu jednoho URL si připravím funkci check_url. Ta zadané URL zkontroluje HTTP požadavkem HEAD (z balíčku httr), zjistí návratový kód a vrátí tibble s původním URL, výsledným URL (z toho se pozná případné přesměrování) a kódem odpovědi. Pro požadavek se také nastaví timeout v sekundách. Pokud server do této doby neodpoví, místo výsledného HTTP kódu se zapíše NA.\r\n\r\n\r\ncheck_url <- function(url, timeout) {\r\n  resp <- try(HEAD(url, timeout(timeout)), silent = TRUE)\r\n  if (class(resp) == \"try-error\") {\r\n    status <- NA_integer_\r\n    dest_url <- NA_character_\r\n  } else {\r\n    status <- resp$status_code\r\n    dest_url <- resp$url\r\n  }\r\n  tibble(url, dest_url, status)\r\n}\r\n\r\n\r\n\r\nVyzkouším, zda funkce funguje s platným (ale přesměrovaným) URL.\r\n\r\n\r\ncheck_url(\"https://wikipedia.org/\", 1)\r\n\r\n\r\n\r\n\r\n{\"columns\":[{\"label\":[\"url\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"dest_url\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"status\"],\"name\":[3],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"https://wikipedia.org/\",\"2\":\"https://www.wikipedia.org/\",\"3\":\"200\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\r\n  \r\n\r\nA raději i s neplatným:\r\n\r\n\r\ncheck_url(\"https://www.wikipedia.org/iououoiuoiuoiu\", 1)\r\n\r\n\r\n\r\n\r\n{\"columns\":[{\"label\":[\"url\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"dest_url\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"status\"],\"name\":[3],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"https://www.wikipedia.org/iououoiuoiuoiu\",\"2\":\"https://en.wikipedia.org/iououoiuoiuoiu\",\"3\":\"404\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\r\n  \r\n\r\nKontrola celého seznamu URL\r\nA teď již mohu pomocí funkce map_dfr z balíčku purrr zkontrolovat celý seznam URL. Zároveň si budu pomocí funkcí tic a toc z balíčku tictoc měřit, jak dlouho to celé trvá s timeoutem nastaveným na 0.5 sekundy. Reálně by byl potřeba vyšší timeout, např. 3 sekundy, ale Wikiepedia je docela rychlá a já chci ukázat výstup, ve kterém se některá URL v časovém limitu zkontrolovat nepodařilo.\r\n\r\n\r\ntic()\r\nstatus_codes <- urls |>\r\n  map_dfr(check_url, 0.5)\r\ntoc()\r\n\r\n\r\n60.52 sec elapsed\r\n\r\nTrvá to docela dlouho a mohlo by to trvat ještě déle, pokud by byl server pomalejší. Teoreticky až počet URL v seznamu krát timeout. Tak dlouho se mi čekat nechce.\r\nProto raději zkusím balíček furrr, který nabízí obdobné funkce jako purrr, jenže paralelizované tak, aby využily víc jader a vláken procesoru. Natavím 6 vláken, takže načtení URL by mělo být skoro šestkrát rychlejší.\r\nZrychlení balíčkem furrr\r\n\r\n\r\nplan(multisession, workers = 6)\r\n\r\ntic()\r\nstatus_codes <- urls |>\r\n  future_map_dfr(check_url, 0.5)\r\ntoc()\r\n\r\n\r\n13.34 sec elapsed\r\n\r\nJo! Šestkrát rychlejší to sice není, ale i tak je zrychlení super. S tím už se pár tisíc URL zpracovat dá.\r\nZobrazení výsledků\r\nA zbývá se podívat na výsledky. Jsou v dataframu (tibble), takže stačí běžné funkce z balíčku dplyr\r\nSouhrnný přehled\r\n\r\n\r\nstatus_codes |>\r\n  count(status, sort = TRUE)\r\n\r\n\r\n\r\n\r\n{\"columns\":[{\"label\":[\"status\"],\"name\":[1],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"n\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"200\",\"2\":\"324\"},{\"1\":\"404\",\"2\":\"10\"},{\"1\":\"NA\",\"2\":\"3\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\r\n  \r\n\r\nVadné URL\r\n\r\n\r\nstatus_codes |> \r\n  filter(status != 200)\r\n\r\n\r\n\r\n\r\n{\"columns\":[{\"label\":[\"url\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"dest_url\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"status\"],\"name\":[3],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"https://www.wikipedia.org/HmPsw2WtYSxSgZ6\",\"2\":\"https://en.wikipedia.org/HmPsw2WtYSxSgZ6\",\"3\":\"404\"},{\"1\":\"https://www.wikipedia.org/tF2KxtgdzehXaH9\",\"2\":\"https://en.wikipedia.org/tF2KxtgdzehXaH9\",\"3\":\"404\"},{\"1\":\"https://www.wikipedia.org/xtgn1TlDJE8PPM9\",\"2\":\"https://en.wikipedia.org/xtgn1TlDJE8PPM9\",\"3\":\"404\"},{\"1\":\"https://www.wikipedia.org/8ESGr2Rn7YC7ktN\",\"2\":\"https://en.wikipedia.org/8ESGr2Rn7YC7ktN\",\"3\":\"404\"},{\"1\":\"https://www.wikipedia.org/f5NHoRoonRkdi0T\",\"2\":\"https://en.wikipedia.org/f5NHoRoonRkdi0T\",\"3\":\"404\"},{\"1\":\"https://www.wikipedia.org/DNbL6FfPm6QztsA\",\"2\":\"https://en.wikipedia.org/DNbL6FfPm6QztsA\",\"3\":\"404\"},{\"1\":\"https://www.wikipedia.org/8eLeJBm5SVbKUxT\",\"2\":\"https://en.wikipedia.org/8eLeJBm5SVbKUxT\",\"3\":\"404\"},{\"1\":\"https://www.wikipedia.org/tubP9vI3wi8YxaP\",\"2\":\"https://en.wikipedia.org/tubP9vI3wi8YxaP\",\"3\":\"404\"},{\"1\":\"https://www.wikipedia.org/eJJDMz958gctfjW\",\"2\":\"https://en.wikipedia.org/eJJDMz958gctfjW\",\"3\":\"404\"},{\"1\":\"https://www.wikipedia.org/eomyRJP0BqEE4Fj\",\"2\":\"https://en.wikipedia.org/eomyRJP0BqEE4Fj\",\"3\":\"404\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\r\n  \r\n\r\nTimeouty\r\nA pokud tam jsou i adresy, které nestihly timeout, pak jdou vypsat takhle:\r\n\r\n\r\nstatus_codes |> \r\n  filter(is.na(status))\r\n\r\n\r\n\r\n\r\n{\"columns\":[{\"label\":[\"url\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"dest_url\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"status\"],\"name\":[3],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"https://ru.wikipedia.org/\",\"2\":\"NA\",\"3\":\"NA\"},{\"1\":\"https://sd.wikipedia.org/\",\"2\":\"NA\",\"3\":\"NA\"},{\"1\":\"https://ig.wikipedia.org/\",\"2\":\"NA\",\"3\":\"NA\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\r\n  \r\n\r\nPřípadně je můžu znovu projet s vyšším timeoutem, třeba takhle:\r\n\r\n\r\nstatus_codes |> \r\n  filter(is.na(status)) |> \r\n  pull(url) |> \r\n  future_map_dfr(check_url, 2)\r\n\r\n\r\n\r\nA to je všechno :-)\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-04-10T12:01:08+02:00",
    "input_file": {}
  }
]
