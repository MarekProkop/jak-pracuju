---
title: "Kontrola stavových kódů většího počtu URL v R"
description: |
  Potřeboval jsem rychle zkontrolovat stavové kódy HTTP dlouhého seznamu URL. Přitom jsem si vyzkoušel balíček furrr na paralelní zpracování a moc se mi zalíbil.
author:
  - name: Marek Prokop
    affiliation: PROKOP software s.r.o.
    affiliation_url: https://www.prokopsw.cz/cs
date: 2022-04-09
output:
  distill::distill_article:
    self_contained: false
---

```{r setup, include=FALSE}
paged_print <- function(x, ...) {
  if (inherits(x, "data.frame")) {
    knitr::knit_print(rmarkdown::paged_table(x), ...)
  } else {
    knitr::knit_print(x, ...)
  }
}

kable_print <- function(x, ...) {
  knitr::knit_print(knitr::kable(x), ...)
}

knitr::opts_chunk$set(
  echo = TRUE,
  render = paged_print
)

set.seed(123)
```

Klient nedávno měnil CMS i doménu webu, došlo ke změně hodně URL, klasický zmatek, jako v těhle případech vždy. Asi měsíc po změně se ukázalo, že se něco nepovedlo a URL starého webu, které měly být přesměrované na nový web, nyní vrací chybu. Navíc jsou některé z nich ještě stále ve výsledcích hledání Googlu.

Vytáhli jsme tedy ze Search Console starého webu URL všech stránek, které se od změny domény alespoň jednou zobrazily, a mým úkolem bylo rychle zkontrolovat, jaké HTTP kódy vracejí. Konkrétně mě zajímalo, která URL fungují (vrací HTTP 200), neexistují (vrací chybu 404 nebo jinou 4xx), nebo na serveru způsobí nějakou chybu (kódy 5xx).

```{r warning=FALSE, message=FALSE}
library(tidyverse)
library(furrr)
library(rvest)
library(httr)
library(tictoc)
```

Pro potřeby tohoto zápisku jsem skutečná URL nahradil odkazy z úvodní stránky Wikipedie, ke kterým jsem navíc přidal 10 náhodných adres, aby mi to ukázalo nějaké chyby. Na principu to nic nemění. Ty odkazy jsem získal takhle:

```{r}
start_url <- "https://www.wikipedia.org/"

urls <- read_html(start_url) |>
  html_elements("a") |>
  html_attr("href") |>
  xml2::url_absolute(start_url) |> 
  c(paste0(start_url, stringi::stri_rand_strings(10, 15)))
```

Jedná se o `r length(urls)` adres, a náhodný vzorek deseti z nich vypadá takhle:

```{r}
sample(urls, 10)
```

## Kontrola jednoho URL

Pro kontrolu jednoho URL si připravím funkci `check_url`. Ta zadané URL zkontroluje HTTP požadavkem HEAD (z balíčku [httr](https://httr.r-lib.org/)), zjistí návratový kód a vrátí tibble s původním URL, výsledným URL (z toho se pozná případné přesměrování) a kódem odpovědi. Pro požadavek se také nastaví timeout v sekundách. Pokud server do této doby neodpoví, místo výsledného HTTP kódu se zapíše `NA`.

```{r}
check_url <- function(url, timeout) {
  resp <- try(HEAD(url, timeout(timeout)), silent = TRUE)
  if (class(resp) == "try-error") {
    status <- NA_integer_
    dest_url <- NA_character_
  } else {
    status <- resp$status_code
    dest_url <- resp$url
  }
  tibble(url, dest_url, status)
}
```

Vyzkouším, zda funkce funguje s platným (ale přesměrovaným) URL.

```{r layout="l-body-outset"}
check_url("https://wikipedia.org/", 1)
```

A raději i s neplatným:

```{r layout="l-body-outset"}
check_url("https://www.wikipedia.org/iououoiuoiuoiu", 1)
```

## Kontrola celého seznamu URL

A teď již mohu pomocí funkce `map_dfr` z balíčku [purrr](https://purrr.tidyverse.org/) zkontrolovat celý seznam URL. Zároveň si budu pomocí funkcí `tic` a `toc` z balíčku [tictoc](http://collectivemedia.github.io/tictoc/) měřit, jak dlouho to celé trvá s timeoutem nastaveným na 0.5 sekundy. Reálně by byl potřeba vyšší timeout, např. 3 sekundy, ale Wikiepedia je docela rychlá a já chci ukázat výstup, ve kterém se některá URL v časovém limitu zkontrolovat nepodařilo.

```{r}
tic()
status_codes <- urls |>
  map_dfr(check_url, 0.5)
toc()
```

Trvá to docela dlouho a mohlo by to trvat ještě déle, pokud by byl server pomalejší. Teoreticky až počet URL v seznamu krát timeout. Tak dlouho se mi čekat nechce.

Proto raději zkusím balíček [furrr](https://furrr.futureverse.org/), který nabízí obdobné funkce jako *purrr*, jenže paralelizované tak, aby využily víc jader a vláken procesoru. Natavím 6 vláken, takže načtení URL by mělo být skoro šestkrát rychlejší.

### Zrychlení balíčkem *furrr*

```{r}
plan(multisession, workers = 6)

tic()
status_codes <- urls |>
  future_map_dfr(check_url, 0.5)
toc()
```

Jo! Šestkrát rychlejší to sice není, ale i tak je zrychlení super. S tím už se pár tisíc URL zpracovat dá.

## Zobrazení výsledků

A zbývá se podívat na výsledky. Jsou v dataframu (tibble), takže stačí běžné funkce z balíčku [dplyr](https://dplyr.tidyverse.org/)

### Souhrnný přehled

```{r}
status_codes |>
  count(status, sort = TRUE)
```

### Vadné URL

```{r layout="l-body-outset"}
status_codes |> 
  filter(status != 200)
```

### Timeouty

A pokud tam jsou i adresy, které nestihly timeout, pak jdou vypsat takhle:

```{r layout="l-body-outset"}
status_codes |> 
  filter(is.na(status))
```

Případně je můžu znovu projet s vyšším timeoutem, třeba takhle:

```{r echo=TRUE, eval=FALSE}
status_codes |> 
  filter(is.na(status)) |> 
  pull(url) |> 
  future_map_dfr(check_url, 2)
```

A to je všechno :-)
